================================================================================
二、需求分析
================================================================================

需求分析是软件开发的基础和前提，直接影响系统设计和实现的质量。本章通过对企业在大语言模型应用过程中的实际调研和深入分析，明确系统需要实现的功能和性能指标，为后续的系统设计和开发提供依据。

（一）系统业务总体需求

通过对企业在大语言模型应用过程中的调研，结合实际业务场景分析，本系统需要满足以下总体业务需求：

1. 统一的模型管理平台
企业在使用大语言模型时，往往需要同时对接多个提供商。例如，使用OpenAI的GPT系列进行通用对话，使用DeepSeek进行推理任务，同时在内网部署Ollama或vLLM运行开源模型。这导致技术人员需要在OpenAI控制台、DeepSeek后台、本地命令行等多个平台之间频繁切换，管理效率低下。

本系统需要提供统一的模型管理平台，让企业用户能够在一个界面中：
- 管理来自不同提供商的所有模型配置
- 查看各个模型的状态和可用性
- 统一设置和调整模型参数
- 对比不同模型的性能和成本

这不仅提升了管理效率，也降低了操作失误的风险。

2. 便捷的模型配置与测试
传统的LLM接入方式需要编写代码，这对非技术人员（如产品经理、业务运营人员）构成了障碍。即使是技术人员，在尝试新模型或调整参数时，也需要修改代码、重启服务，效率较低。

本系统需要提供图形化的配置界面，让用户可以：
- 通过表单填写API端点、密钥、模型名称等配置信息
- 可视化调整temperature、max_tokens、top_p等生成参数
- 一键测试配置是否正常工作，立即获得反馈
- 保存多套配置方案，快速切换使用

这种"零代码"的配置方式显著降低了大语言模型的使用门槛。

3. 多模型效果对比
在模型选型阶段，企业需要评估不同模型在具体业务场景下的表现。传统做法是逐个模型进行测试，然后人工记录和对比结果，过程繁琐且容易遗漏细节。

本系统需要提供多模型并行对比测试功能：
- 同时选择2-3个模型，输入相同的问题
- 并行请求各个模型，实时展示它们的回答
- 自动记录响应时间、token消耗等关键指标
- 支持批量测试，一次性对多个问题进行测试
- 导出对比报告，辅助决策

通过直观的并排对比，企业可以快速找到最适合业务需求的模型。

4. 对话历史管理
在日常使用大语言模型的过程中，会产生大量有价值的对话记录。这些记录可能包含重要的业务信息、有效的问题解决方案、优秀的模型输出案例等。如果不加以保存和管理，这些知识就会丢失。

本系统需要提供完善的对话历史管理功能：
- 自动保存所有对话记录，无需用户手动操作
- 支持创建多个会话，分别管理不同主题的对话
- 提供搜索功能，快速查找历史记录
- 支持导出对话记录为Markdown或JSON格式，便于归档和分享
- 允许重新发送历史消息，复用之前的对话场景

通过系统化的历史管理，企业可以积累和沉淀LLM使用经验。

5. 提示词知识库
提示词（Prompt）的质量直接影响大语言模型的输出效果。在实际使用中，企业会逐步总结出一些高质量的提示词模板，例如用于客服回复的标准话术模板、用于代码生成的任务描述模板等。这些提示词是宝贵的知识资产。

本系统需要提供提示词知识库功能：
- 支持创建和保存提示词模板
- 按用途分类管理（如角色扮演、代码生成、文案创作等）
- 支持添加标签，便于搜索和筛选
- 在对话时快速选择和应用提示词模板
- 统计提示词使用频次，识别最有价值的模板
- 支持团队内共享提示词，促进知识复用

通过建立提示词知识库，企业可以将个人经验转化为团队资产，避免重复试错。

6. 模型训练支持
虽然通用大语言模型能力强大，但在特定领域或专业场景下，通过微调（Fine-tuning）可以获得更好的效果。例如，医疗企业可以基于医疗数据微调模型，使其更准确地回答医学问题；金融企业可以基于金融数据微调模型，使其更好地理解金融术语和业务逻辑。

本系统需要集成模型训练能力：
- 提供便捷的训练任务配置界面，无需直接操作命令行
- 支持上传训练数据集（JSON、JSONL、CSV等格式）
- 支持选择训练方法（LoRA、QLoRA、全参数微调等）
- 提供训练进度监控，实时查看损失曲线、学习率变化等指标
- 管理训练产出的模型，支持加载和使用微调后的模型

通过集成训练功能，企业可以在保障数据隐私的前提下，训练专属的领域模型。

7. 权限与安全控制
企业应用系统必须具备完善的权限和安全控制机制。不同用户可能有不同的职责和权限，需要进行相应的访问控制。同时，系统需要保护用户数据和敏感信息（如API密钥）的安全。

本系统需要实现：
- 用户注册和登录功能，验证用户身份
- 区分普通用户和管理员角色，实施不同的权限策略
- 数据隔离，确保用户只能访问自己的数据
- 密码加密存储，API密钥安全管理
- 审计日志，记录关键操作，便于追溯

安全机制的完善是系统在企业环境中应用的基础保障。

8. 良好的用户体验
用户体验直接影响系统的使用率和用户满意度。一个操作繁琐、界面丑陋、响应缓慢的系统，即使功能完善，也难以被用户接受。

本系统需要在用户体验方面达到以下要求：
- 界面美观：采用现代化的UI设计，色彩搭配合理，布局清晰
- 操作流畅：页面切换自然，无卡顿；交互响应及时，无延迟
- 反馈及时：操作成功或失败立即给出提示；长时间操作显示进度
- 个性化设置：支持亮色/暗色主题切换，适应不同使用场景和个人偏好
- 智能辅助：自动生成会话标题，推荐常用提示词，减少用户重复操作
- 容错性强：对用户的误操作给予友好提示，避免数据丢失

良好的用户体验能够让用户更愿意使用系统，从而充分发挥系统的价值。

以上八个方面构成了本系统的总体业务需求。这些需求来源于企业在大语言模型应用过程中的真实痛点，具有明确的业务价值和实际意义。

（二）系统功能需求分析

根据总体业务需求，本系统划分为以下七大功能模块：用户管理、模型配置管理、对话与测试、系统提示词管理、训练任务管理、训练可视化、系统管理。以下对每个模块的功能需求进行详细分析。

1. 用户管理需求

用户管理是系统安全和权限控制的基础。本系统需要提供完整的用户生命周期管理功能，包括注册、登录、信息维护、权限控制等。

（1）用户注册与登录
用户注册是用户使用系统的第一步。系统需要收集用户的基本信息，建立用户账号。具体需求如下：

- 注册信息收集：用户注册时需要提供邮箱地址、昵称和密码。邮箱作为用户的唯一标识，昵称用于界面显示。
- 数据验证：邮箱需要符合标准邮箱格式（如包含@符号和有效域名），昵称长度限制在2-20个字符之间，密码长度至少8位且必须包含字母和数字，以保证安全性。
- 唯一性检查：系统需要检查邮箱和昵称的唯一性，避免重复注册。如果邮箱或昵称已被使用，应给出明确提示。
- 密码加密存储：密码不能以明文形式存储在数据库中，必须使用bcrypt等安全的哈希算法进行加密。bcrypt具有自适应性，可以抵御暴力破解攻击。

登录功能需要验证用户的身份凭证，并返回访问令牌。具体需求如下：

- 凭证验证：用户输入邮箱和密码，系统验证邮箱是否存在、密码是否正确。
- JWT令牌生成：验证成功后，生成两个令牌：
  * Access Token（访问令牌）：有效期较短（如15分钟），用于API请求的身份认证。令牌中包含用户ID、角色等信息。
  * Refresh Token（刷新令牌）：有效期较长（如7天），用于刷新Access Token。Refresh Token通过HttpOnly Cookie返回，防止被JavaScript访问，提高安全性。
- 记住登录状态：用户可以选择"记住我"选项，此时Refresh Token的有效期延长至7天或更长，用户在此期间无需重新登录。
- 登录失败处理：如果邮箱不存在或密码错误，返回统一的错误提示"邮箱或密码错误"，避免泄露账号是否存在的信息。

（2）密码管理
密码管理功能帮助用户在忘记密码或需要修改密码时进行相应操作。

- 密码重置：用户可以通过邮箱验证的方式重置密码。具体流程为：
  1. 用户输入注册邮箱，系统发送验证码到该邮箱
  2. 用户输入验证码和新密码
  3. 系统验证验证码的有效性（通常5-10分钟内有效）
  4. 验证通过后更新密码
  
- 密码修改：已登录用户可以在个人设置中修改密码。为了安全，需要先验证旧密码，然后设置新密码。新密码同样需要满足强度要求。

- 密码强度要求：密码长度至少8位，必须包含字母和数字的组合。建议包含大小写字母、数字和特殊字符，以提高安全性。系统在用户输入密码时实时显示密码强度（弱/中/强），引导用户设置安全密码。

（3）用户信息管理
用户在使用过程中可能需要更新个人信息。系统需要提供信息编辑功能。

- 个人资料编辑：用户可以修改昵称、邮箱、头像等个人信息。昵称和邮箱的唯一性检查仍然适用。头像支持上传图片文件（JPG、PNG格式），系统自动调整尺寸并存储。
  
- 账号信息查看：用户可以查看自己的账号创建时间、最后登录时间、当前角色（普通用户/管理员）等信息，了解账号状态。

- 数据导出：用户可以导出自己的数据（包括对话记录、配置信息等），实现数据可携带。

（4）权限控制
系统采用基于角色的访问控制（RBAC），区分普通用户和管理员两种角色。

- 角色定义：
  * 普通用户（user）：可以使用系统的所有核心功能，包括配置模型、进行对话、查看训练任务等，但只能访问和管理自己创建的数据。
  * 管理员（admin）：除了普通用户的所有权限外，还具有管理其他用户、查看系统统计、修改系统配置等特权。

- 权限验证：每个API请求都需要验证用户的JWT令牌，确认用户身份。对于需要管理员权限的操作（如用户管理、系统配置），还需要额外验证用户角色。

- 数据隔离：普通用户只能查看和操作自己创建的数据。例如，用户A创建的对话会话、模型配置、训练任务等，用户B无法访问。管理员可以查看所有用户的数据，但修改操作仍需谨慎。

- 管理员操作：
  * 查看所有用户列表，包括用户的注册时间、最后登录时间、活跃度等统计信息
  * 调整用户角色，将普通用户提升为管理员，或将管理员降级为普通用户
  * 禁用或启用用户账号。禁用后，用户无法登录和使用系统
  * 查看用户的活动日志，追踪异常操作

2. 模型配置需求

模型配置管理是系统的核心功能之一。企业需要接入多个LLM提供商，配置不同的模型，并灵活调整参数。

（1）模型提供商管理
系统预置了8个主流的LLM提供商，涵盖本地部署和云端API两种类型。

- 本地部署提供商：
  * Ollama：开源的本地LLM运行工具，支持LLaMA、Mistral、Gemma等多种开源模型。优点是数据完全本地化，无需外网访问，适合对数据隐私要求高的场景。
  * vLLM：高性能的LLM推理引擎，专为生产环境优化。支持PagedAttention等先进技术，吞吐量高，适合大并发场景。提供OpenAI兼容的API接口。
  * SGLang：另一个高性能的本地推理引擎，支持结构化生成和约束解码，适合需要格式化输出的场景。

本地提供商的特点是无需API密钥，只需配置本地服务的端点地址（如http://localhost:11434）。

- 云端API提供商：
  * OpenAI：全球领先的AI公司，提供GPT-5、o1和o3系列等强大的模型。其中o3模型在2025年底发布，在推理任务上表现卓越。优点是能力强大、响应稳定，缺点是价格较高。
  * DeepSeek：国内领先的AI公司，提供DeepSeek-V3、DeepSeek-R1等模型。DeepSeek-R1具有强大的推理能力，适合复杂问题的解决。
  * 硅基流动（SiliconFlow）：提供多种开源模型的API服务，如Qwen、GLM等，价格相对OpenAI更低。
  * 302.AI：聚合多个AI服务的平台，提供统一的API接口。
  * 智谱AI（Zhipu）：清华大学孵化的AI公司，提供GLM-5等模型，中文能力强，在2025年持续迭代升级。

云端提供商需要配置API密钥（API Key），用于身份认证和计费。

- 提供商配置项：
  * 提供商ID：唯一标识，如"openai"、"ollama"、"deepseek"
  * API端点（Endpoint）：API服务的地址，如https://api.openai.com/v1
  * API密钥（API Key）：用于认证，仅云端提供商需要
  * 是否本地提供商：标识是否为本地部署，影响API密钥的必填性
  * 支持的模型列表：该提供商下可用的模型

- 配置管理：管理员可以启用或禁用某些提供商，也可以为提供商设置全局的API密钥（用户可以选择使用全局密钥或自己的密钥）。

（2）模型列表刷新
不同的LLM提供商支持的模型数量和类型不断更新。例如，OpenAI可能发布新的GPT模型，Ollama可能增加对新开源模型的支持。系统需要能够动态获取最新的模型列表。

- 自动获取模型列表：系统调用提供商的API（如OpenAI的/v1/models端点），获取当前可用的模型列表。对于支持此功能的提供商，系统定期刷新模型列表。

- 模型信息展示：模型列表显示以下信息：
  * 模型名称：如"gpt-5-preview"、"deepseek-chat"、"claude-4-opus"、"qwen-3-72b"
  * 模型能力：对话、推理、代码生成、多模态（支持图像输入）等
  * 参数规模：如7B、13B、70B，帮助用户了解模型的复杂度
  * 上下文长度：模型支持的最大Token数，如4K、8K、128K、200K。上下文长度影响模型能处理的输入和输出长度。
  * 是否可用：部分模型可能需要特殊权限或额外付费，系统标识其可用状态

- 手动添加模型：对于不支持自动获取模型列表的提供商，或用户需要添加自定义模型时，系统支持手动输入模型名称和参数。

（3）模型配置CRUD操作
CRUD是Create（创建）、Read（读取）、Update（更新）、Delete（删除）的缩写，代表数据管理的基本操作。

- 创建模型配置（Create）：
  用户在系统中创建一个新的模型配置，需要提供以下信息：
  * 选择提供商（从预置的8个提供商中选择）
  * 选择或输入模型名称
  * 配置生成参数：
    - temperature（温度）：控制输出的随机性，取值0-2。值越小输出越确定，值越大输出越随机。通常对话任务使用0.7-1.0，代码生成使用0.2-0.5。
    - top_p（核采样）：控制输出的多样性，取值0-1。与temperature配合使用，影响候选词的选择范围。
    - max_tokens：限制单次生成的最大Token数。过大可能导致成本增加，过小可能截断输出。
    - presence_penalty（存在惩罚）：降低模型重复话题的倾向
    - frequency_penalty（频率惩罚）：降低模型重复用词的倾向
  * 可选的系统提示词：为该配置设置默认的系统提示词
  * 配置名称和备注：便于识别和管理

  系统在保存配置前，进行参数校验，确保所有参数在合法范围内。

- 读取模型配置（Read）：
  用户可以查看自己创建的所有模型配置，以列表或卡片形式展示。每个配置显示提供商、模型名称、主要参数、创建时间等信息。支持按提供商、模型名称、创建时间等条件筛选和排序。

- 更新模型配置（Update）：
  用户可以编辑已有的模型配置，修改任何参数（提供商和模型名称除外，如需更换需创建新配置）。修改后的配置立即生效，后续的对话会使用新参数。

  特殊情况：如果API密钥发生变更（如续费、密钥泄露后更换），用户需要及时更新配置中的API Key。

- 删除模型配置（Delete）：
  用户可以删除不再使用的模型配置。为了防止误删，系统需要二次确认。如果该配置被某些对话会话使用，系统应提示用户，避免影响现有会话。支持批量删除多个配置。

（4）配置测试
创建或修改模型配置后，用户需要验证配置是否正确。如果API密钥错误、端点地址不可达、模型名称拼写错误等，会导致配置无法使用。

- 测试消息发送：用户点击"测试"按钮，系统使用该配置向模型发送一条测试消息（如"Hello"或"你好"）。

- 连接验证：系统尝试连接提供商的API端点，验证网络连通性和API密钥的有效性。

- 响应检查：如果模型成功返回响应，说明配置正常，系统显示"测试成功"及返回的内容。如果请求失败，系统捕获错误信息（如401 Unauthorized、404 Not Found、超时等），并向用户展示具体的错误原因，帮助用户定位问题。

- 性能测试：测试过程中记录响应时间，帮助用户了解该模型的响应速度，作为选择模型的参考依据之一。

（5）默认配置管理
为了提升用户体验，系统提供默认配置机制。

- 系统初始化默认配置：系统首次启动时，自动创建默认的管理员账号（admin@example.com / admin123）和一个示例模型配置（如Ollama的某个模型），让用户可以快速体验系统功能。

- 用户默认模型设置：用户可以将某个模型配置设置为默认配置。创建新的对话会话时，自动选中默认配置，无需每次手动选择，减少重复操作。

- 默认参数模板：系统预设几套常用的参数模板（如"创意写作"模板使用高temperature，"代码生成"模板使用低temperature），用户可以基于模板快速创建配置。

3. 对话与测试需求

对话功能是用户与大语言模型交互的主要方式，也是系统最核心、使用频率最高的功能。本系统需要提供流畅、高效、功能丰富的对话体验。

（1）智能对话功能
智能对话是系统的核心功能，需要满足以下需求：

- 多会话管理：用户可以创建多个独立的对话会话，每个会话有自己的上下文和历史记录。例如，用户可以在一个会话中讨论技术问题，在另一个会话中进行创意写作，两个会话互不干扰。会话数量不限制，但系统会对长期未使用的会话进行归档提示。

- 流式输出：大语言模型生成长文本时，如果等待全部生成完毕再显示，用户体验较差。系统采用流式输出（Streaming）技术，模型生成的文本逐字逐句地实时传输到前端并显示，类似打字机效果。这不仅提升了用户体验，也让用户能够提前看到部分回答，在不满意时可以及时停止生成。

  流式输出的技术实现：后端使用Server-Sent Events（SSE）协议，将模型的逐个Token传输到前端。前端接收到数据后，实时追加到对话框中。为了保证流畅性，前端使用虚拟滚动和渲染优化技术，即使对话内容很长也不会卡顿。

- 思维链识别与展示：部分先进的大语言模型（如DeepSeek-R1、OpenAI的o1系列）在生成答案前会输出推理过程，这些推理过程通常用特殊标签`<think></think>`包裹。系统需要能够识别这些标签，并将推理过程和最终答案分离展示。

  具体展示方式：
  * 推理过程（`<think>`标签内的内容）以灰色、小字号、可折叠的形式显示，让用户知道模型是"如何思考的"
  * 最终答案（标签外的内容）以正常样式显示
  * 用户可以点击展开/折叠推理过程，需要时查看详细推理步骤

  思维链功能帮助用户理解模型的决策逻辑，在模型回答错误时，可以通过分析推理过程找到错误的根源。

- 模型选择：每个会话可以选择使用哪个模型配置。用户可以在会话设置中切换模型，切换后的新消息使用新模型，历史消息保持不变。系统记录每条消息使用的模型，便于后续分析对比。

- 系统提示词应用：系统提示词（System Prompt）定义了模型的角色和行为规范。用户在创建会话或发送消息时，可以选择应用某个提示词模板。例如，选择"Python编程助手"模板后，模型会以Python专家的身份回答问题，代码质量更高。

- 多模态支持（扩展功能）：对于支持多模态的模型（如GPT-5 Vision、Claude 4 Vision、Qwen-3-VL），用户可以上传图片，让模型分析图片内容。系统需要处理图片的上传、编码（Base64）、传输等流程。

（2）会话管理
会话管理帮助用户组织和维护多个对话。

- 会话列表展示：在侧边栏或独立页面显示用户的所有会话，每个会话显示：
  * 会话标题（自动生成或用户自定义）
  * 创建时间和最后更新时间
  * 消息数量（如"15条消息"）
  * 使用的模型图标或名称
  * 会话状态（活跃、归档等）

  会话列表支持按时间倒序、按标题搜索、按模型筛选等功能。

- 会话标题生成：新创建的会话默认标题为"新对话"。当用户发送第一条消息后，系统可以自动根据消息内容生成有意义的标题（如消息是"如何学习Python"，标题可以是"Python学习建议"）。用户也可以手动修改标题。

  自动生成标题的实现：可以调用LLM生成简短的标题，或使用关键词提取算法。

- 会话重命名：用户可以随时修改会话标题，便于识别和查找。标题长度限制在50个字符以内。

- 会话删除：用户可以删除不再需要的会话。删除前系统弹出确认对话框，防止误删。删除后会话及其所有消息从数据库中永久删除（或移入回收站，保留30天）。支持批量删除多个会话。

- 清空会话：清空当前会话的所有消息，但保留会话本身。适用于想重新开始对话但保留会话设置的场景。

- 会话导出：用户可以将会话的所有消息导出为文件，格式包括：
  * Markdown格式：适合阅读和分享，保留消息的格式（标题、列表、代码块等）
  * JSON格式：适合程序处理，包含完整的元数据（时间戳、模型名称、角色等）
  * TXT纯文本格式：适合简单场景

  导出功能便于用户归档重要对话，或分享给团队成员。

（3）历史记录
历史记录功能帮助用户回顾和复用过去的对话。

- 完整历史查看：用户可以查看会话的完整对话历史，包括所有用户提问和模型回答。历史记录以时间顺序展示，每条消息显示发送时间、角色（用户/助手）、内容。

- 历史消息搜索：用户可以在历史记录中搜索关键词，快速定位到包含特定内容的消息。搜索支持全文匹配和高亮显示。

- 消息复制：用户可以复制任意消息的内容，方便粘贴到其他地方使用。对于代码块，提供"一键复制代码"按钮。

- 消息重新发送：用户可以选择历史消息中的某条用户提问，重新发送给模型。这在以下场景有用：
  * 模型回答不满意，想换个模型重新回答
  * 模型参数调整后，想看新参数下的回答效果
  * 网络中断导致消息未成功发送，重新发送

- 消息编辑（扩展功能）：用户可以编辑已发送的消息内容，修改后重新提交。这在发现输入有误时很有用。

（4）多模型对比测试
多模型对比测试是系统的特色功能，帮助用户科学评估不同模型的性能。

- 模型选择：用户可以同时选择2-3个模型配置进行对比测试。选择过多会影响界面展示和响应速度，因此限制在3个以内。

- 并行请求：系统同时向选中的所有模型发送相同的问题，各模型独立并行处理，互不影响。并行请求比串行请求更快，用户可以更快看到所有结果。

- 实时对比展示：前端界面分栏展示各个模型的回答，每栏显示模型名称、回答内容。回答以流式方式同步展示，用户可以实时看到各模型的生成速度差异。

- 批量测试：对于需要大量测试的场景（如模型选型、参数调优），用户可以上传一个问题列表（TXT或JSON格式），系统自动对每个问题进行多模型对比测试，并记录所有结果。

  批量测试流程：
  1. 用户上传包含10个问题的文件
  2. 系统逐个发送给选中的3个模型
  3. 记录每个问题、每个模型的回答、响应时间、Token消耗
  4. 生成对比报告，以表格或图表形式展示

- 指标记录：系统自动记录以下对比指标：
  * 响应时间：从发送请求到收到第一个Token的时间（首字节时间，TTFB）和总响应时间
  * Token消耗：输入Token数和输出Token数，用于成本估算
  * 回答质量：用户可以对每个回答进行评分（1-5星），主观评价质量
  * 错误率：记录哪些模型返回了错误或无法理解问题

- 对比报告导出：系统生成对比报告，包含：
  * 各模型的回答内容
  * 性能指标对比表格
  * 图表展示（如响应时间对比柱状图、Token消耗对比饼图）
  * 用户评分统计

  报告可以导出为PDF或HTML格式，便于决策者审阅。

多模型对比测试功能使模型选型从主观判断变为数据驱动，提高了决策的科学性。

4. 系统提示词管理需求

系统提示词（System Prompt）是影响模型输出质量的关键因素。提示词管理功能帮助用户积累和复用优秀的提示词模板。

（1）提示词创建与编辑
- 创建提示词模板：用户可以创建新的提示词模板，输入以下信息：
  * 模板名称：简短的名称，如"Python专家"、"营销文案生成器"
  * 模板内容：提示词的具体文本，可以包含多行，支持Markdown格式
  * 分类：选择提示词的用途分类（见下文）
  * 标签：添加自定义标签，如"编程"、"创意"、"客服"等
  * 是否公开：标记为公开的提示词可以被其他用户查看和使用（适合团队协作场景）
  * 描述：对提示词用途和效果的简要说明

- Markdown编辑器：提示词编辑界面提供Markdown编辑器，支持加粗、斜体、列表、代码块等格式。编辑器提供实时预览功能，用户可以边写边看效果。

- 模板变量：提示词支持变量占位符，如{user_input}、{context}、{date}等。使用时，系统自动将变量替换为实际值。例如：
  你是一个专业的{role}，请根据以下背景信息：{context}，回答用户的问题：{user_input}
  使用时，用户填写role为"律师"，context为案件背景，系统自动生成完整的提示词。

- 格式验证：系统验证提示词的格式和长度，过长的提示词（如超过8000字符）会警告用户可能超出模型的上下文限制。

- 提示词编辑：用户可以随时编辑已保存的提示词模板，修改内容、分类、标签等。编辑会保留版本历史，方便回溯。

（2）提示词分类管理
为了便于查找和使用，系统对提示词进行分类管理。

- 预置分类：系统预设以下常用分类：
  * 角色扮演：定义模型扮演特定角色（如老师、医生、律师、心理咨询师）
  * 代码生成：用于代码编写、调试、优化等任务
  * 文案创作：用于营销文案、广告语、产品描述等创意写作
  * 数据分析：用于数据解读、报表生成、趋势分析
  * 翻译润色：用于文本翻译、语法检查、风格调整
  * 知识问答：用于特定领域的问答（如医学、法律、技术）
  * 教育辅导：用于教学、答疑、习题讲解
  * 其他：不属于以上分类的提示词

- 自定义分类：用户可以创建自己的分类，满足特殊需求。

- 分类筛选：在提示词列表中，用户可以按分类筛选，快速找到目标提示词。

（3）提示词复用
提示词复用是提示词管理的核心价值。

- 快速选择：在对话界面，用户点击"选择提示词"按钮，弹出提示词选择器，显示所有可用的提示词模板。模板按分类或使用频次排序，用户点击即可应用。

- 提示词组合：部分场景需要组合多个提示词。例如，组合"角色扮演"提示词和"输出格式"提示词。系统支持选择多个提示词，自动拼接成完整的系统消息。

- 使用频次统计：系统记录每个提示词被使用的次数，在列表中显示使用次数。高频提示词可以标记为"热门"或"推荐"，方便新用户参考。

- 提示词收藏：用户可以收藏常用的提示词，收藏的提示词在选择器中优先展示，提高使用效率。

- 团队共享：在团队版或企业版系统中，管理员可以创建全局提示词模板，所有用户都可以使用。这促进了最佳实践的共享，避免每个人重复造轮子。

（4）提示词效果评估（扩展功能）
- A/B测试：对于同一个任务，用户可以创建两个不同的提示词，分别测试，对比效果，选择更优的提示词。

- 效果反馈：用户可以对提示词的效果进行评分和评论，形成提示词的质量评价体系。

通过系统化的提示词管理，用户可以将提示词工程（Prompt Engineering）的成果沉淀为可复用的资产，持续提升模型使用效果。

5. 训练任务管理需求

模型训练是将通用大语言模型适配到特定业务场景的重要手段。本系统集成LLaMA-Factory训练框架，为用户提供便捷的模型微调能力。

（1）训练任务配置
训练任务配置是启动训练的前提，用户需要指定训练的各项参数。

- 基础模型选择：用户从支持的基础模型列表中选择一个作为起点。LLaMA-Factory支持100+种预训练模型，包括LLaMA、Qwen、GLM、Mistral、Gemma等。不同模型有不同的参数规模和特性，用户根据业务需求和硬件条件选择。

  系统提供模型推荐功能：根据用户的GPU显存（如24GB、40GB）和任务类型（对话、代码生成等），推荐合适的模型和训练方法。

- 训练数据集上传：用户需要准备训练数据集。数据集格式要求：
  * Alpaca格式：包含instruction、input、output三个字段，适合指令微调任务
  * ShareGPT格式：包含多轮对话记录，适合对话系统训练
  * JSON/JSONL格式：每行或每个对象是一条训练样本
  * CSV格式：表格形式，适合简单数据

  系统支持用户上传本地文件（限制大小如100MB以内），并进行格式验证。如果格式不正确，给出明确的错误提示和修正建议。

- 训练参数配置：用户配置以下训练超参数：
  * 学习率（learning_rate）：控制参数更新的步长，通常为1e-5到5e-4之间。学习率过大可能导致训练不稳定，过小则收敛缓慢。
  * 批次大小（batch_size）：每次训练使用的样本数量。批次越大训练越稳定，但需要更多显存。系统根据GPU显存自动推荐合适的批次大小。
  * 训练轮数（num_epochs）：遍历全部数据的次数。通常2-5个epoch即可，过多可能导致过拟合。
  * 梯度累积步数（gradient_accumulation_steps）：用于模拟更大的批次，降低显存需求。
  * 最大序列长度（max_seq_length）：限制输入的最大Token数，影响显存占用。
  * 保存策略：每隔多少步保存一次检查点，以及是否只保存最优模型。

- 训练方法选择：用户选择训练方法：
  * LoRA（Low-Rank Adaptation）：参数高效微调方法，只训练少量新增的低秩矩阵，显存需求低，训练速度快。适合大多数场景。
  * QLoRA：在LoRA基础上使用4-bit量化，进一步降低显存需求。在消费级GPU（如RTX 3090 24GB）上可以微调较大的模型（如LLaMA-70B）。
  * Full Fine-tuning（全参数微调）：训练模型的全部参数，效果最好但显存需求极高，仅适合有专业GPU（如A100）的场景。

  系统为每种方法提供默认参数模板，用户可以在此基础上微调。

- 训练任务命名和描述：用户为训练任务指定一个名称（如"客服对话模型v1.0"）和描述（说明训练目的、数据来源等），便于后续管理和追溯。

- 配置验证：系统在启动训练前验证配置的合法性，包括：
  * 数据集是否存在且格式正确
  * 训练参数是否在合理范围内
  * 硬件资源（GPU显存、磁盘空间）是否满足需求
  * 基础模型文件是否已下载

  如果验证失败，给出具体的错误信息和解决方案。

（2）任务执行控制
训练任务启动后，用户需要能够监控和控制任务的执行。

- 启动训练：用户点击"开始训练"按钮，系统执行以下步骤：
  1. 生成LLaMA-Factory的训练配置文件（YAML或JSON格式）
  2. 调用LLaMA-Factory的CLI或API启动训练进程
  3. 将训练任务记录到数据库，状态设为"训练中"
  4. 开始收集训练日志和指标

- 实时日志查看：训练过程中，LLaMA-Factory会输出日志信息（如当前epoch、step、loss值等）。系统实时捕获这些日志，并在Web界面展示。用户可以在不登录服务器的情况下，查看训练进度。

  日志展示要求：
  * 滚动显示最新的日志行，支持手动滚动查看历史日志
  * 关键信息（如loss、learning_rate）高亮显示
  * 错误信息用红色标注，便于快速发现问题

- 暂停/恢复训练（扩展功能）：对于长时间的训练任务，用户可能需要暂停训练（如释放GPU资源给其他任务），稍后再恢复。系统支持保存训练状态（检查点），恢复时从上次的进度继续训练，而不是从头开始。

- 终止训练：如果发现训练配置有误、数据集质量不佳或训练效果不理想，用户可以提前终止训练。系统发送终止信号给训练进程，清理临时文件，将任务状态更新为"已终止"。

（3）任务状态管理
系统需要记录和展示训练任务的状态，便于用户了解任务进展。

- 任务状态定义：
  * 待执行（pending）：任务已创建但尚未开始，可能在等待资源或排队
  * 训练中（running）：任务正在执行，GPU正在工作
  * 已完成（completed）：训练正常结束，模型已保存
  * 失败（failed）：训练过程中出现错误（如GPU显存不足、数据格式错误），自动终止
  * 已终止（stopped）：用户手动终止了训练

- 任务时间记录：系统记录任务的创建时间、启动时间、结束时间，计算训练耗时。这有助于用户评估训练效率，优化配置。

- 模型文件路径保存：训练完成后，LLaMA-Factory会将微调后的模型保存到指定目录（如`./output/llama-lora-model`）。系统记录该路径，用户可以在后续对话中加载和使用该模型。

- 任务列表展示：在训练管理页面，展示所有训练任务的列表，每个任务显示：
  * 任务名称和描述
  * 基础模型和训练方法
  * 任务状态和进度（如"训练中 - 40%完成"）
  * 创建时间和耗时
  * 操作按钮（查看日志、查看指标、加载模型等）

  列表支持按状态筛选（如只看"训练中"的任务）、按时间排序等。

（4）LLaMA-Factory集成
系统与LLaMA-Factory深度集成，提供无缝的训练体验。

- 自动启动LLaMA-Factory Web UI：系统启动时，自动启动LLaMA-Factory的Gradio Web界面（默认端口7860），用户可以在系统中通过iframe或新标签页访问LLaMA-Factory的原生界面，使用其高级功能。

- API调用：系统通过LLaMA-Factory的命令行接口（CLI）或API调用训练功能。配置参数通过JSON或YAML文件传递，避免复杂的命令行参数拼接。

- 配置同步：用户在系统界面中配置的训练参数，自动同步到LLaMA-Factory的配置文件（如`./examples/train_lora.yaml`），保证配置的一致性。

- 模型管理：LLaMA-Factory训练完成后的模型文件，系统自动索引并显示在模型管理界面。用户可以将训练好的模型作为新的模型配置添加到系统中，用于对话测试。

6. 训练可视化需求

模型训练过程产生大量的指标数据（如loss、learning_rate、accuracy等），需要可视化工具帮助用户直观地了解训练状态和效果。本系统集成SwanLab作为可视化工具。

（1）SwanLab服务管理
SwanLab是一个开源的训练监控工具，需要在服务器上运行其可视化服务。

- 启动SwanLab服务：用户点击"启动SwanLab"按钮，系统执行以下操作：
  1. 检查SwanLab是否已安装（通过pip或conda）
  2. 检查训练日志目录是否存在数据
  3. 使用`swanlab watch`命令启动可视化服务，监控指定的日志目录
  4. 检测服务是否成功启动（通过访问SwanLab的默认端口5092）
  5. 返回SwanLab的访问地址（如http://localhost:5092），用户可以在浏览器中打开

- 停止SwanLab服务：用户不再需要查看训练数据时，可以停止服务以释放系统资源。系统向SwanLab进程发送终止信号，优雅地关闭服务。

- 配置SwanLab端口和目录：系统允许用户配置SwanLab的端口号（默认5092）和监控的数据目录（默认`./swanlab`）。这在多用户或多任务场景下避免端口冲突。

- 服务状态检测：系统定期检测SwanLab服务的运行状态（通过HTTP请求健康检查端点），并在界面上显示状态（运行中/已停止）。如果服务意外崩溃，及时通知用户。

（2）项目管理
SwanLab以项目（Project）为单位组织训练数据，一个项目可以包含多个训练实验（Run）。

- 查看SwanLab项目列表：系统调用SwanLab的API或解析日志目录，获取所有项目的列表。每个项目显示项目名称、创建时间、包含的实验数量。

- 项目训练历史：用户点击某个项目，查看该项目下的所有训练实验。每个实验显示实验名称、训练时间、最终loss值、最佳指标等。

- 清理过期数据：训练日志和指标数据会占用磁盘空间。系统提供清理功能，用户可以删除不再需要的旧项目或实验，释放空间。清理前系统弹出确认提示，防止误删。

（3）实时监控
训练过程中，实时监控关键指标有助于及时发现问题。

- 实时损失曲线：SwanLab自动记录每个训练步的loss值，并绘制损失曲线。用户可以在Web界面中看到loss随训练步数的变化趋势。理想情况下，loss应该逐渐下降并趋于稳定。如果loss不降反升或波动剧烈，说明训练配置可能有问题。

- 学习率变化监控：许多训练策略会动态调整学习率（如学习率预热、余弦退火等）。SwanLab记录并展示学习率的变化曲线，帮助用户验证学习率调度器是否按预期工作。

- 训练进度跟踪：SwanLab显示当前的epoch数、step数、预计剩余时间等信息。用户可以估算训练何时完成，合理安排时间。

- GPU利用率监控（扩展功能）：如果SwanLab配置了系统监控，还可以查看GPU利用率、显存占用、CPU利用率等硬件指标，判断资源是否被充分利用。

（4）历史数据分析
训练完成后，用户需要分析历史数据，评估训练效果。

- 对比不同训练任务：SwanLab支持在同一图表中对比多个实验的指标。例如，用户可以对比不同学习率、不同批次大小下的训练效果，选择最优的超参数组合。

  对比维度包括：
  * 最终loss值：哪个配置收敛得更好
  * 收敛速度：哪个配置更快达到目标loss
  * 训练稳定性：哪个配置的loss曲线更平滑

- 导出训练指标数据：SwanLab支持将训练指标导出为CSV或JSON格式，用户可以进一步使用Excel、Python等工具进行分析，或生成自定义报表。

- 生成训练报告：系统自动生成训练报告，包含：
  * 训练配置概要（模型、数据集、超参数）
  * 训练时长和资源消耗
  * 关键指标的最终值和曲线图
  * 训练过程中的异常或警告
  * 建议和改进方向

  报告可以导出为PDF或HTML，便于分享和存档。

通过SwanLab的可视化能力，用户可以深入了解训练过程，及时调整策略，提升训练效果。

7. 系统管理需求

系统管理模块为管理员提供全局的管理和监控能力，保障系统的稳定运行。

（1）仪表盘功能
仪表盘（Dashboard）是管理员了解系统状态的首页，提供全局概览。

- 系统概览统计：仪表盘显示以下核心指标：
  * 用户数量：总用户数、活跃用户数（近7天登录）、今日新增用户
  * 模型配置数量：已配置的模型总数、各提供商的配置分布
  * 对话会话数：总会话数、今日新增会话、最活跃的会话
  * 训练任务数：总任务数、进行中的任务数、已完成的任务数

  这些指标以数字卡片、环形图、柱状图等形式展示，一目了然。

- 最近活动展示：显示系统的最近活动，如：
  * 最新创建的用户和登录记录
  * 最新的对话会话和消息
  * 最近启动或完成的训练任务

  活动列表帮助管理员快速了解系统的使用情况。

- 系统资源监控：监控服务器的资源使用情况：
  * CPU使用率：当前CPU的平均使用率和各核心的使用率
  * 内存占用：已使用内存、可用内存、内存占用百分比
  * 磁盘空间：各分区的已用空间、可用空间，数据库文件大小、日志文件大小

  如果资源使用率过高（如内存占用超过90%），系统发出警告，提醒管理员及时处理。

- 异常和错误提示：如果系统检测到异常（如API调用失败率过高、数据库连接失败、训练任务频繁失败），在仪表盘上醒目显示警告信息，引导管理员排查问题。

（2）用户管理（管理员专用）

管理员需要管理系统的所有用户，维护用户秩序。

- 查看用户列表：管理员可以查看所有用户的列表，每个用户显示：
  * 用户ID、邮箱、昵称
  * 注册时间、最后登录时间
  * 当前角色（普通用户/管理员）
  * 账号状态（正常/禁用）
  * 活跃度统计（如登录次数、对话会话数）

  列表支持搜索（按邮箱或昵称）、筛选（按角色或状态）、排序（按注册时间或活跃度）。

- 调整用户角色：管理员可以将普通用户提升为管理员，或将管理员降级为普通用户。角色调整后立即生效，影响用户的权限。

- 禁用/启用账号：对于违规用户或测试账号，管理员可以禁用其账号。禁用后，用户无法登录，已登录的会话失效。管理员也可以重新启用被禁用的账号。

- 查看用户活动统计：管理员可以查看每个用户的详细活动统计，包括：
  * 对话会话数量和消息数量
  * 使用过的模型类型
  * API调用次数和成本估算
  * 训练任务数量

  这些统计帮助管理员了解用户的使用习惯，优化系统配置和资源分配。

- 批量操作：管理员可以批量禁用、启用或删除多个用户，提高管理效率。

（3）系统配置
系统配置模块允许管理员调整系统的全局设置。

- 修改系统标题和Logo：管理员可以自定义系统的名称（如"XX企业AI平台"）和Logo图片，实现品牌化。

- 配置默认模型和提示词：管理员可以设置系统级的默认模型配置和提示词模板，新用户首次使用时自动应用这些默认值，降低上手难度。

- 设置Token有效期：管理员可以调整Access Token和Refresh Token的有效期。缩短有效期提高安全性，延长有效期提升用户体验，需要根据企业安全策略平衡。

- 配置文件上传限制：设置允许上传的文件类型（如只允许图片、文档、数据集等特定格式）和最大文件大小（如单个文件不超过100MB），防止滥用存储资源。

- CORS和安全设置：配置跨域资源共享（CORS）的允许域名列表，设置API访问频率限制（Rate Limiting），防止恶意攻击。

（4）日志管理
完善的日志系统是排查问题和审计操作的基础。

- 系统运行日志：记录系统的启动、关闭、重启等事件，以及各个模块的运行状态。日志级别包括DEBUG、INFO、WARNING、ERROR。

- API调用日志：记录所有API请求，包括请求时间、请求路径、请求参数、响应状态码、响应时间、用户身份等。API日志用于性能分析和安全审计。

- 错误日志：单独记录ERROR级别的日志，包含异常类型、堆栈跟踪、发生时间、影响范围等。错误日志帮助开发人员快速定位和修复bug。

- 用户操作日志：记录用户的关键操作，如登录、注销、修改密码、创建/删除配置、启动训练任务等。操作日志用于审计和追溯。

- 日志查看和筛选：管理员可以在Web界面中查看日志，支持按时间范围、日志级别、关键词筛选。日志以分页形式展示，避免一次加载过多数据。

- 日志导出：管理员可以将日志导出为文本文件，便于离线分析或提交给技术支持。

- 日志轮转和清理：日志文件会不断增长，系统需要定期轮转日志（如每天生成一个新日志文件）和清理旧日志（如保留最近30天的日志），防止占用过多磁盘空间。

（三）非功能性需求

非功能性需求定义了系统在性能、安全性、可维护性、可扩展性、易用性等方面的质量属性。这些需求虽然不直接体现在功能层面，但对系统的成功运行和用户满意度至关重要。

1. 性能需求

性能是影响用户体验的关键因素。本系统需要在以下方面满足性能要求：

（1）响应时间要求
- 页面加载时间：
  * 首屏加载时间<2秒（在标准网络环境下，100Mbps带宽）
  * 后续页面切换<500ms
  * 使用代码分割、懒加载等技术优化首屏性能
  * 静态资源启用CDN加速和浏览器缓存

- API响应时间：
  * 普通请求（如获取用户信息、模型配置列表）：平均响应时间<300ms，P95响应时间<500ms
  * 数据库查询：单表查询<100ms，关联查询<300ms
  * 流式响应首字节时间（TTFB）：<500ms，确保用户快速看到模型的初始输出
  * 文件上传响应：小文件(<10MB)<2秒，大文件(<100MB)<30秒

- 模型调用响应：
  * 本地模型（Ollama、vLLM）首字节时间：<1秒（取决于模型大小和GPU性能）
  * 云端API首字节时间：<2秒（受网络延迟影响）
  * 流式输出帧率：每秒至少30个Token，保证流畅的阅读体验

（2）并发处理能力
- 用户并发：
  * 支持至少200个并发用户同时在线
  * 支持至少50个用户同时进行对话（流式请求）
  * 支持至少10个并发训练任务（取决于GPU资源）

- 请求处理能力：
  * API服务器QPS（每秒查询数）：普通请求>500 QPS，流式请求>100 QPS
  * 数据库并发连接数：支持至少100个并发连接
  * 使用连接池技术复用数据库连接，降低连接开销

- 负载均衡：
  * 支持水平扩展，通过Nginx或负载均衡器分发请求到多个后端实例
  * 单个训练任务不影响其他用户正常使用，训练进程与API服务隔离

（3）资源占用
- 后端服务：
  * 无训练任务时内存占用<1GB
  * 有训练任务时内存占用根据模型大小动态调整（7B模型LoRA训练约需10-15GB显存）
  * CPU占用：空闲时<10%，处理请求时瞬时峰值<80%

- 前端资源：
  * 打包后的JS/CSS文件总大小<5MB（gzip压缩后）
  * 首屏加载资源<1MB，使用异步加载非关键资源
  * 运行时内存占用<200MB（浏览器标签页）

- 数据库：
  * SQLite数据库文件大小控制在合理范围（<1GB），支持定期清理历史数据
  * 支持切换到MySQL或PostgreSQL应对大数据量场景

- 磁盘空间：
  * 日志文件总大小<1GB，定期轮转和清理
  * 训练产出的模型文件妥善管理，提供清理建议

（4）缓存策略
- 前端缓存：
  * 静态资源（JS、CSS、图片）启用强缓存，缓存时间1年
  * API响应启用协商缓存，减少不必要的请求
  * 使用Service Worker实现离线缓存（PWA扩展功能）

- 后端缓存：
  * 模型列表、提供商配置等不常变化的数据使用内存缓存（如Redis），缓存时间10分钟
  * 用户权限信息缓存，减少数据库查询
  * 训练任务状态使用短期缓存（1分钟），平衡实时性和性能

2. 安全性需求

安全是企业系统的生命线。本系统需要在多个层面保障安全。

（1）身份认证
- JWT双令牌机制：
  * Access Token有效期15分钟，用于API身份认证，包含用户ID、角色、过期时间等声明
  * Refresh Token有效期7天，用于刷新Access Token，通过HttpOnly Cookie存储，防止XSS攻击窃取
  * 令牌使用HS256或RS256算法签名，防止篡改

- 密码安全：
  * 密码使用bcrypt算法加密存储，成本因子设为12，计算时间约100ms，抵御暴力破解
  * 禁止存储明文密码或可逆加密的密码
  * 密码强度要求：至少8位，包含字母和数字，建议包含特殊字符

- 会话管理：
  * 用户登出时立即失效Refresh Token，防止会话劫持
  * 检测到异常登录（如IP地址突变、设备指纹变化）时要求重新认证
  * 限制同一账号的并发会话数（如最多3个设备同时登录）

（2）权限控制
- 基于角色的访问控制（RBAC）：
  * 明确定义普通用户和管理员的权限边界
  * API接口验证JWT令牌的有效性和用户角色
  * 敏感操作（如删除用户、修改系统配置）需要管理员权限

- 数据隔离：
  * 普通用户只能访问自己创建的数据（对话会话、模型配置、训练任务）
  * 数据库查询自动添加用户ID过滤条件，防止越权访问
  * 管理员查看其他用户数据时记录审计日志

- 最小权限原则：
  * 数据库用户权限最小化，只授予必要的增删改查权限
  * API密钥等敏感信息加密存储，仅在需要时解密使用

（3）数据安全
- API密钥管理：
  * API Key在数据库中加密存储（使用AES-256或Fernet加密）
  * 传输过程使用HTTPS加密，防止中间人攻击窃取
  * 支持用户自主更换API密钥，旧密钥立即失效

- 数据备份：
  * 定期自动备份数据库（如每日凌晨备份）
  * 备份文件加密存储，防止泄露
  * 提供数据恢复功能，应对数据丢失或损坏

- 敏感数据脱敏：
  * 日志中不记录密码、API密钥等敏感信息
  * 错误提示不暴露系统内部结构或敏感数据

（4）防攻击措施
- CORS跨域限制：
  * 配置允许的来源域名列表，拒绝未授权的跨域请求
  * 生产环境严格限制CORS策略，只允许可信域名

- SQL注入防护：
  * 使用ORM（SQLAlchemy）的参数化查询，避免拼接SQL语句
  * 对用户输入进行验证和转义

- XSS防护：
  * 前端对用户输入进行HTML转义，防止注入恶意脚本
  * 使用Content Security Policy（CSP）限制脚本来源

- CSRF防护：
  * Refresh Token的Cookie设置SameSite=Lax或Strict属性，防止CSRF攻击
  * 关键操作（如修改密码）要求验证原密码或短信验证码

- 限流和防爆破：
  * API接口实施速率限制（Rate Limiting），如登录接口限制为每分钟5次尝试
  * 密码错误5次后锁定账号15分钟，防止暴力破解
  * 对频繁请求的IP地址进行封禁

3. 可维护性需求

可维护性影响系统的长期运维成本和迭代速度。

（1）代码规范
- 编码标准：
  * 后端代码遵循PEP 8规范（Python官方风格指南）
  * 前端代码遵循Vue官方风格指南和ESLint规则
  * 变量、函数、类命名采用见名知意的方式（如`get_user_by_id`而非`gubi`）

- 代码注释：
  * 公共API和复杂逻辑必须有详细注释
  * 函数和类使用文档字符串（Docstring）说明参数、返回值、异常
  * 关键算法和业务逻辑添加行内注释，解释设计意图

- 代码审查：
  * 新功能和bug修复需要通过代码审查（Code Review）
  * 使用Git分支管理，主分支保护，禁止直接提交

（2）模块化设计
- 前后端分离：
  * 前端和后端独立开发、测试、部署
  * 通过RESTful API[11]通信，接口定义清晰

- 分层架构：
  * 后端分为路由层、业务逻辑层、数据访问层，职责明确
  * 前端分为组件层、状态管理层、API调用层

- 低耦合高内聚：
  * 模块间依赖最小化，便于独立修改和测试
  * 核心功能抽象为独立模块（如LLM客户端、JWT工具），便于复用

（3）文档完善
- API文档：
  * 使用FastAPI自动生成Swagger/OpenAPI文档
  * 文档包含接口路径、请求方法、参数说明、响应示例、错误码

- 部署文档：
  * README文件详细说明系统架构、技术栈、部署步骤
  * 提供Docker Compose一键部署方案
  * 列举常见问题和解决方案（FAQ）

- 设计文档：
  * 关键模块提供设计文档，说明设计思路和实现方案
  * 数据库设计文档包含E-R图和表结构说明

（4）日志记录
- 分级日志：
  * DEBUG：调试信息，仅开发环境启用
  * INFO：正常操作记录（如用户登录、启动训练）
  * WARNING：警告信息（如API调用超时重试）
  * ERROR：错误信息（如数据库连接失败、模型调用异常）

- 日志内容：
  * 包含时间戳、日志级别、模块名称、消息内容
  * 错误日志包含堆栈跟踪（Traceback），便于定位问题
  * 记录关键操作的上下文信息（如用户ID、请求ID）

- 日志管理：
  * 使用日志轮转，每日生成新文件或按大小分割
  * 保留最近30天的日志，定期清理旧日志

4. 可扩展性需求

系统需要具备良好的可扩展性，以应对未来的功能增强和规模增长。

（1）架构可扩展
- 水平扩展：
  * 前后端无状态设计，支持部署多个实例
  * 使用Nginx或负载均衡器分发流量
  * 会话状态存储在Redis等外部存储，不依赖单实例内存

- 数据库扩展：
  * SQLite适合中小规模，大规模场景可切换到MySQL或PostgreSQL
  * 数据库连接通过配置文件管理，切换数据库无需修改代码
  * 支持读写分离和分库分表（未来扩展）

- 微服务化（未来方向）：
  * 核心功能（如用户管理、模型调用、训练管理）可拆分为独立微服务
  * 使用消息队列（如RabbitMQ、Kafka）实现服务间通信

（2）功能可扩展
- 插件机制：
  * 新增LLM提供商只需添加配置和客户端类，无需修改核心代码
  * 客户端类继承`BaseClient`基类，实现标准接口

- 配置驱动：
  * 系统行为通过配置文件控制（如环境变量、YAML文件）
  * 避免硬编码，便于调整和定制

- API版本管理：
  * API路径包含版本号（如`/api/v1/users`），新版本不影响旧版本
  * 支持多版本并存，平滑升级

（3）数据可扩展
- 数据库迁移：
  * 使用Alembic管理数据库版本，支持平滑升级
  * 新增字段或表时生成迁移脚本，自动应用到数据库

- 兼容性设计：
  * 新增字段设置默认值或允许NULL，保证向后兼容
  * 废弃字段保留一段时间，给予用户迁移缓冲期

5. 易用性需求

易用性直接影响用户的学习成本和使用效率。

（1）界面友好
- 现代化设计：
  * 采用Element Plus等成熟UI框架，界面美观、组件丰富
  * 色彩搭配协调，符合用户审美习惯
  * 布局清晰，功能分区明确

- 响应式设计：
  * 支持不同分辨率的PC端（1920x1080、1366x768等）
  * 使用相对单位（rem、em、%）和媒体查询实现自适应
  * 未来可扩展移动端适配

- 暗色模式：
  * 提供亮色和暗色两种主题，适应不同使用场景
  * 用户可以自由切换，选择保存到本地存储
  * 暗色模式降低长时间使用的眼睛疲劳

（2）操作便捷
- 快捷键支持：
  * 对话输入框支持Ctrl+Enter快捷发送
  * 常用功能提供键盘快捷键（如Ctrl+S保存配置）

- 智能推荐：
  * 自动生成会话标题，减少用户命名负担
  * 推荐常用提示词模板，提高效率
  * 根据用户历史使用记录，推荐合适的模型配置

- 表单校验：
  * 实时校验用户输入，及时提示错误（如邮箱格式错误、密码强度不足）
  * 提交前再次校验，防止无效请求
  * 错误提示清晰具体，指导用户如何修正

（3）反馈及时
- 操作反馈：
  * 操作成功/失败立即弹出消息提示（Toast或Notification）
  * 消息自动消失（如3秒后），不干扰用户继续操作
  * 关键操作（如删除）需要二次确认，防止误操作

- 进度提示：
  * 长时间操作（如文件上传、模型训练启动）显示进度条或加载动画
  * 流式对话显示打字机动画，提示模型正在生成
  * 异步任务（如训练）提供实时进度百分比

- 错误处理：
  * 错误提示清晰，说明错误原因和建议解决方案
  * 避免技术术语，使用用户能理解的语言
  * 提供帮助链接或联系方式，便于用户求助

（4）多端适配
- 浏览器兼容：
  * 支持主流浏览器的最新两个版本（Chrome、Firefox、Edge、Safari）
  * 使用Babel等工具转译ES6+语法，保证兼容性
  * 对不支持的浏览器给出升级提示

- 设备适配：
  * 当前版本主要面向PC端
  * 预留移动端适配空间（响应式布局、触摸事件）

通过满足以上非功能性需求，系统不仅能够正常运行，还能在性能、安全、可维护性、可扩展性、易用性等方面达到企业级应用的标准，为用户提供优质的使用体验。

本章通过对企业大语言模型应用的深入调研和分析，从业务总体需求、功能需求、非功能性需求三个层面全面阐述了系统需要实现的目标。这些需求涵盖了用户管理、模型配置、对话测试、提示词管理、训练任务、可视化、系统管理等七大功能模块，以及性能、安全、可维护性、可扩展性、易用性等五个质量维度。需求分析的详尽和具体，为后续的系统设计和实现提供了坚实的基础。

