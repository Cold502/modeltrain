五、系统测试

平台系统开发完成以后，在部署试运行之前要进行测试，一方面是为了发现系统设计中的错误和漏洞，另一方面也是为了通过测试分析来改进软件开发的质量。测试可以按照单元分别测试，还需要在集成完毕后进行集成测试，最后是验收测试。本次测试将以超级管理员的身份进入系统，该账户的账号admin@example.com，密码为admin123。当输入错误的密码或者账号并不能进入系统，只有当正确的账号密码才能进入主页，所以登录功能通过测试。输入正确的账号密码能够正常跳转到主页，主页显示通过测试。

本文中也对开发的模块做了初步测试，主要集中在功能测试上，采用黑盒测试法。由于本系统中有单元测试、集成测试、性能测试等多个部分，分别进行系统测试。

（一）测试环境

系统测试需要配置适当的硬件和软件环境，以确保测试结果的准确性和可重复性。

1. 硬件环境

测试服务器采用Intel Core i7处理器、32GB内存、512GB SSD固态硬盘的配置。为了支持本地大模型推理测试，服务器配备了NVIDIA RTX 3060显卡（12GB显存）。网络环境采用千兆局域网连接，确保数据传输的稳定性。

客户端测试机包括Windows 11、macOS和Ubuntu操作系统，用于验证系统的跨平台兼容性。

2. 软件环境

表5-1 测试环境软件配置
软件名称                   版本信息                    用途说明
操作系统                   Ubuntu/Windows 11          服务器/客户端运行环境
Python                     3.10+                       后端开发语言
FastAPI                    0.104+                      Web框架
SQLite                     3.40+                       数据库
Node.js                    20.x LTS                    前端构建环境
Vue.js                     3.x                         前端框架
Pytest                     7.x                         单元测试框架
Locust                     2.x                         性能测试工具
Postman                    10.x                        接口测试工具
OWASP ZAP                  2.14+                       安全测试工具

（二）单元测试

单元测试是对软件中的最小可测试单元进行检查和验证，确保每个模块都能正常工作。本系统使用Pytest测试框架进行单元测试，对核心功能模块进行了详细测试。

1. 用户认证模块测试

用户认证模块是系统的基础模块，负责用户注册、登录、令牌管理等功能。测试内容包括JWT令牌生成与验证、密码哈希与校验、登录流程、令牌刷新等功能。

表5-2 用户认证模块测试用例
测试用例编号    测试内容                    输入数据                    预期结果            实际结果
TC-AUTH-001    JWT令牌生成                 用户ID、过期时间            生成有效令牌        通过
TC-AUTH-002    JWT令牌验证                 有效令牌                    返回用户信息        通过
TC-AUTH-003    JWT令牌过期验证             过期令牌                    返回过期错误        通过
TC-AUTH-004    密码哈希生成                明文密码                    生成bcrypt哈希      通过
TC-AUTH-005    密码哈希验证（正确）        正确密码                    验证通过            通过
TC-AUTH-006    密码哈希验证（错误）        错误密码                    验证失败            通过
TC-AUTH-007    用户登录（正确凭据）        正确邮箱密码                返回令牌            通过
TC-AUTH-008    用户登录（错误密码）        正确邮箱错误密码            返回错误提示        通过
TC-AUTH-009    用户登录（不存在用户）      不存在的邮箱                返回错误提示        通过
TC-AUTH-010    令牌刷新                    有效Refresh Token           返回新Access Token  通过

2. 模型配置模块测试

模型配置模块负责管理大语言模型的配置信息，包括创建、读取、更新、删除配置等操作。

表5-3 模型配置模块测试用例
测试用例编号    测试内容                    输入数据                    预期结果            实际结果
TC-CFG-001     创建模型配置                配置名称、提供商等          配置创建成功        通过
TC-CFG-002     获取配置列表                用户令牌                    返回配置列表        通过
TC-CFG-003     获取单个配置                配置ID                      返回配置详情        通过
TC-CFG-004     更新模型配置                配置ID、新数据              配置更新成功        通过
TC-CFG-005     删除模型配置                配置ID                      配置删除成功        通过
TC-CFG-006     设置默认配置                配置ID                      设为默认成功        通过
TC-CFG-007     刷新模型列表                提供商信息                  返回模型列表        通过
TC-CFG-008     配置权限验证                其他用户令牌                返回权限错误        通过

3. LLM客户端模块测试

LLM客户端模块负责与不同的大语言模型提供商进行通信，支持OpenAI、Ollama、vLLM等多种提供商。

表5-4 LLM客户端模块测试用例
测试用例编号    测试内容                    输入数据                    预期结果            实际结果
TC-LLM-001     客户端工厂模式              提供商类型                  返回对应客户端      通过
TC-LLM-002     OpenAI客户端创建            API配置信息                 客户端创建成功      通过
TC-LLM-003     Ollama客户端创建            本地服务地址                客户端创建成功      通过
TC-LLM-004     非流式对话请求              消息列表                    返回完整响应        通过
TC-LLM-005     流式对话请求                消息列表                    返回流式响应        通过
TC-LLM-006     API异常处理                 无效API密钥                 返回错误信息        通过
TC-LLM-007     超时处理                    超长响应时间                返回超时错误        通过

4. 对话会话模块测试

对话会话模块负责管理用户与模型的对话，包括会话创建、消息保存、历史记录管理等功能。

表5-5 对话会话模块测试用例
测试用例编号    测试内容                    输入数据                    预期结果            实际结果
TC-CHAT-001    创建对话会话                会话标题、模型配置          会话创建成功        通过
TC-CHAT-002    发送用户消息                会话ID、消息内容            消息保存成功        通过
TC-CHAT-003    保存模型响应                会话ID、响应内容            响应保存成功        通过
TC-CHAT-004    获取会话历史                会话ID                      返回消息列表        通过
TC-CHAT-005    思维链解析                  包含think标签的响应         正确提取思维链      通过
TC-CHAT-006    导出Markdown格式            会话ID                      生成MD文件          通过
TC-CHAT-007    导出JSON格式                会话ID                      生成JSON文件        通过
TC-CHAT-008    删除对话会话                会话ID                      会话删除成功        通过

5. 测试覆盖率

使用pytest-cov工具生成测试覆盖率报告，统计代码测试覆盖情况。

表5-6 单元测试覆盖率统计
模块名称                   代码行数        覆盖行数        覆盖率
用户认证模块               256            231            90.2%
模型配置模块               189            165            87.3%
LLM客户端模块              342            295            86.3%
对话会话模块               278            245            88.1%
训练任务模块               156            128            82.1%
系统管理模块               98             82             83.7%
总计                       1319           1146           86.9%

单元测试共执行156个测试用例，通过154个，通过率98.7%。未通过的2个测试用例为边界条件测试，已在后续版本中修复。总体测试覆盖率达到86.9%，超过了80%的目标要求。

（三）集成测试

集成测试是在单元测试的基础上，将所有模块按照设计要求组装成系统进行的测试。本系统的集成测试主要验证各模块之间的接口和数据传递是否正确，业务流程是否能够正常执行。

1. 功能模块测试

表5-7 功能模块集成测试结果
测试模块            测试用例                            测试结果
用户管理            注册-登录-修改资料-登出             通过
                    注册重复邮箱（异常）                通过
                    弱密码注册（异常）                  通过
模型配置            创建-编辑-测试-删除配置             通过
                    刷新模型列表                        通过
                    设置默认配置                        通过
智能对话            创建会话-发送消息-接收回复          通过
                    流式输出显示                        通过
                    思维链解析展示                      通过
                    导出对话记录                        通过
模型对比            选择模型-并行测试-查看结果          通过
                    批量问题测试                        通过
                    导出对比报告                        通过
提示词管理          创建-编辑-应用-删除提示词           通过
                    提示词分类筛选                      通过
训练管理            配置任务-启动-监控-完成             通过
                    SwanLab服务启停                     通过
                    查看训练可视化                      通过
系统管理            查看用户列表                        通过
                    调整用户角色                        通过
                    查看系统日志                        通过
暗色模式            切换主题                            通过
                    刷新后保持主题                      通过

2. 业务流程测试

为了验证系统的完整业务流程，设计了以下端到端测试用例：

（1）新用户完整使用流程测试

测试步骤：访问系统首页→点击注册→填写邮箱、昵称、密码→注册成功跳转登录页→使用新账号登录→进入仪表盘查看系统概览→配置第一个模型（选择Ollama本地模型）→创建对话会话→发送问题接收流式回复→导出对话记录→登出系统。

测试结果：所有步骤执行正常，业务流程完整通过。

（2）管理员用户管理流程测试

测试步骤：管理员登录→进入用户管理页面→查看用户列表→调整某用户角色为管理员→禁用某用户账号→验证被禁用用户无法登录→启用用户账号→验证用户可以正常登录。

测试结果：所有步骤执行正常，权限控制功能完整。

（3）模型对比与选型流程测试

测试步骤：用户登录→配置3个不同的模型→进入模型对比页面→选择3个模型→输入测试问题→并行请求查看实时输出→对比回答质量和响应时间→保存测试记录→导出对比报告。

测试结果：所有步骤执行正常，模型对比功能完整。

（四）性能测试

性能测试验证系统在高负载条件下的响应能力和稳定性。本系统使用Locust性能测试框架进行测试。

1. 用户登录性能测试

测试场景：模拟多用户并发登录系统，测试系统的响应时间和吞吐量。

表5-8 登录并发性能测试结果
并发用户数    平均响应时间(ms)    最大响应时间(ms)    最小响应时间(ms)    成功率(%)    每秒请求数
50            120                250                80                 100          415
100           180                420                95                 100          555
200           280                680                110                99.8         714
500           850                1500               180                98.5         588

测试结论：系统在200并发用户下平均响应时间280ms，满足小于500ms的性能要求。在500并发用户下响应时间有所增加但仍可接受，成功率保持在98%以上。

2. 流式响应性能测试

测试场景：测试模型对话流式响应的首字节延迟和总体响应时间。使用Ollama本地模型，发送100个测试问题，记录首字节时间（TTFB）和完整响应时间。

表5-9 流式响应性能测试结果
性能指标            平均值          最大值          最小值          95百分位值
首字节延迟(ms)      320            580            180            450
完整响应时间(s)     8.5            15.2           3.2            12.3
每秒Token数         42             68             25             55

测试结论：首字节延迟平均320ms，满足小于500ms的要求，用户能够及时看到模型开始响应。流式输出体验流畅，满足实时交互需求。

3. 并发对话性能测试

测试场景：50个用户同时进行流式对话，测试系统资源消耗和响应稳定性。

表5-10 并发对话性能测试结果
性能指标                测试结果
CPU使用率               65%-75%
内存占用                2.8GB
数据库连接数            52个
平均首字节延迟          450ms
测试时长                30分钟
超时/错误次数           0

测试结论：系统支持50个并发流式对话，CPU和内存占用在合理范围内，无超时或错误发生，性能稳定可靠。

（五）其他测试

1. 功能性测试

功能性测试采用黑盒测试方法，验证系统所有功能是否按照需求规格正确实现。

表5-11 功能性测试结果
测试类别            测试项目数        通过数          通过率
核心功能            85               85              100%
辅助功能            45               43              95.6%
界面交互            45               44              97.8%
总计                175              172             98.3%

核心功能包括用户管理、模型配置、对话管理、模型对比、训练管理等，全部通过测试。辅助功能中部分导出格式待优化。界面交互中个别浏览器兼容问题已修复。

2. 可靠性测试

可靠性测试验证系统在异常情况下的容错能力和恢复能力。

表5-12 可靠性测试结果
测试场景                测试方法                            预期结果                测试结果
网络中断测试            对话进行中断开网络                  系统提示连接失败        通过
                        恢复网络后自动重连
数据库连接丢失          模拟数据库锁定                      返回友好错误提示        通过
                        数据库恢复后系统自动恢复
LLM API超时             配置超时时间为5秒                   返回超时错误            通过
                        模拟慢速响应                        用户可重试
长时间运行稳定性        系统连续运行72小时                  无内存泄漏              通过
                                                            无异常崩溃

3. 安全性测试

安全性测试使用OWASP ZAP工具进行扫描，验证系统的安全防护能力。

表5-13 安全性测试结果
测试项目                测试方法                            测试结果
SQL注入测试             在登录表单输入' OR '1'='1           正确拒绝，无漏洞
XSS跨站脚本测试         输入<script>alert('xss')</script>   内容被正确转义
CSRF跨站请求伪造        Cookie使用SameSite=Lax属性          外部站点无法伪造请求
敏感信息泄露            检查API响应                         不包含密码、密钥等
权限控制测试            普通用户访问管理员接口              返回权限错误

安全测试结果表明，系统对常见的Web安全攻击具有良好的防护能力，未发现严重安全漏洞。

4. 兼容性测试

兼容性测试验证系统在不同浏览器和操作系统上的运行情况。

表5-14 浏览器兼容性测试结果
浏览器                  版本            操作系统            测试结果
Chrome                  120.0          Windows/macOS       完全兼容
Firefox                 121.0          Windows/macOS       完全兼容
Edge                    120.0          Windows             完全兼容
Safari                  17.0           macOS               完全兼容

表5-15 分辨率适配测试结果
分辨率                  测试结果
1920x1080               最佳显示效果
2560x1440               完美适配
1366x768                正常显示，部分表格需滚动
3840x2160（4K）         高清显示

移动端测试结果：响应式布局基本可用，建议使用平板或PC端以获得最佳体验。

（六）问题报告

在测试过程中发现了若干问题，经过分析和修复后，系统运行正常。

表5-16 问题报告表
模块位置        问题描述                        问题等级    修正者    采取的动作                  修正时间
对话导出模块    大型对话记录导出较慢            B          开发组    实施分页导出和异步处理      2025/10/15
暗色模式模块    Element Plus组件样式不协调      B          开发组    自定义CSS覆盖和配色调整     2025/10/16
用户认证模块    Safari浏览器Cookie设置问题      B          开发组    增加浏览器检测和兼容配置    2025/10/17

问题等级说明：A级为严重问题，影响系统核心功能；B级为一般问题，影响用户体验但不影响核心功能；C级为轻微问题，属于优化建议。

（七）测试结论

经过全面的单元测试、集成测试、性能测试、安全测试和兼容性测试，对系统进行了综合评估。

表5-17 测试结果汇总
测试类型            测试项目数        通过数          通过率
单元测试            156              154             98.7%
集成测试            28               28              100%
性能测试            6                6               100%
安全测试            5                5               100%
兼容性测试          8                8               100%
总计                203              201             99.0%

表5-18 测试项目总结表
项目名称：基于大模型的企业训练管理平台
测试模块：用户认证、模型配置、对话管理、模型对比、训练管理、系统管理
测试人员：测试组                测试时间：2025年10月            发现问题：3个（已修复）

序号    测试路径                        输入                    输出                    实际结果
1       用户注册登录流程                账号、密码              跳转到系统主页          操作成功
2       模型配置增删改查                配置信息                配置保存成功            操作成功
3       对话消息发送接收                用户消息                模型响应内容            操作成功
4       模型对比测试                    多模型、测试问题        对比结果报告            操作成功
5       训练任务管理                    训练配置                训练进度和结果          操作成功
6       安全性测试                      恶意输入                正确拒绝                操作成功
7       性能测试                        并发请求                响应时间达标            操作成功
8       兼容性测试                      不同浏览器              所有功能正常            操作成功

综上所述，系统功能完整、性能稳定、安全可靠，达到了预期的设计目标。在200并发用户场景下运行流畅，流式响应体验良好，首字节延迟满足要求。安全测试未发现严重漏洞，兼容主流浏览器和操作系统。系统已具备生产环境部署条件，可以投入企业实际使用。
