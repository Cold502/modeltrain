国家开放大学
学 士 学 位 论 文

题目：企业级大语言模型训练管理平台的设计与实现

分部：吉林分部
学习中心：
专业：计算机科学与技术
入学时间：
学号：
姓名：
指导教师：
论文完成日期: 2026 年 02 月

学位论文原创性声明

本人郑重声明：所呈交的学位论文，是本人在导师指导下，进行研究工作所取得的成果。除文中已经注明引用的内容外，本学位论文的研究成果不包含任何他人创作的、已公开发表或者没有公开发表的作品的内容。对本论文所涉及的研究工作做出贡献的其他个人和集体，均已在文中以明确方式标明。本学位论文原创性声明的法律责任由本人承担。

作者签名：              日期：    年   月   日

学位论文版权使用授权声明

本人完全了解国家开放大学关于收集、保存、使用学位论文的规定，同意如下各项内容：按照学校要求提交学位论文的印刷本和电子版本；学校有权保存学位论文的印刷本和电子版，并采用影印、缩印、扫描、数字化或其它手段保存论文；学校有权提供目录检索以及提供本学位论文全文或者部分的阅览服务，以及出版学位论文；学校有权按有关规定向国家有关部门或者机构送交论文的复印件和电子版；在不以赢利为目的的前提下，学校可以适当复制论文的部分或全部内容用于学术活动。

作者签名：              日期：    年   月   日

目  录

摘  要	Ⅰ
一、	综述	1
（一）	系统建设背景	1
（二）	研究意义	1
（三）	术语定义	2
（四）	技术选型	2
1.	后端技术栈	2
2.	前端技术栈	3
3.	数据库与中间件	3
4.	模型训练与监控	4
（五）	系统运行环境	4
1.	软件环境配置	4
2.	硬件环境规格	5
3.	网络环境要求	5

二、	需求分析	6
（一）	系统业务总体需求	6
（二）	系统功能需求分析	6
1.	用户管理需求	6
2.	模型配置需求	7
3.	对话与测试需求	7
4.	系统提示词管理需求	8
5.	训练任务管理需求	8
6.	训练可视化需求	9
7.	系统管理需求	9
（三）	非功能性需求	10
1.	性能需求	10
2.	安全性需求	10
3.	可维护性需求	11
4.	可扩展性需求	11
5.	易用性需求	11

三、	系统总体设计	12
（一）	业务流程图	12
（二）	系统逻辑架构图	13
（三）	系统结构图	13
（四）	技术架构设计	14
1.	体系结构	14
2.	技术总体结构模型图	15
（五）	数据库设计	15
1.	概念模型（E-R图）	15
2.	逻辑模型设计	16
3.	核心业务表设计	17

四、	系统详细设计与实现	20
（一）	后端系统设计与实现	20
1.	用户认证与权限管理模块	20
2.	模型配置管理模块	23
3.	LLM统一调用模块	26
4.	模型对话模块	30
5.	模型对比测试模块	34
6.	系统提示词管理模块	37
7.	训练任务管理模块	39
8.	SwanLab训练可视化模块	41
9.	系统管理模块	43
（二）	前端系统设计与实现	45
1.	前端技术架构与项目结构	45
2.	用户界面布局设计	48
3.	登录注册页面设计与实现	51
4.	模型对话页面设计与实现	54
5.	模型对比测试页面设计与实现	59
6.	模型配置管理页面设计与实现	61
7.	Dify应用管理页面设计与实现	63
8.	暗色模式实现	65
9.	状态管理与路由设计	67
10.	前后端交互机制	70
（三）	系统集成与联调	73
1.	前后端接口联调	73
2.	第三方服务集成	74
3.	跨域配置与代理设置	76

五、	系统测试	77
（一）	测试目的与范围	77
（二）	测试环境	77
（三）	单元测试	78
（四）	集成测试	79
（五）	性能测试	81
（六）	其他测试	83
（七）	测试结论	85

六、	系统部署与运维	86
（一）	部署架构设计	86
（二）	后端部署	86
（三）	前端部署	87
（四）	系统运维	88

七、	结论	89
（一）	系统总结	89
（二）	存在的不足	89
（三）	未来改进方向	90

致  谢	91
参考文献	92
附  录	93

================================================================================
摘  要
================================================================================

自2017年Vaswani等人提出Transformer架构以来[1]，大语言模型（Large Language Models, LLM）技术经历了快速发展阶段。从早期的BERT[2]、GPT系列[3]到新一代的LLaMA系列[4][5]，模型参数规模从数亿级别扩展至千亿级别，在自然语言理解和生成任务上的表现实现了显著提升。2022年底ChatGPT的发布标志着大语言模型进入规模化商业应用阶段，为企业数字化转型提供了新的技术路径[6]。

然而，企业在实际部署和应用大语言模型时面临着诸多技术挑战：
1. **模型选型复杂性**：当前市场上存在OpenAI GPT系列、Anthropic Claude系列、DeepSeek、阿里通义千问、智谱GLM等15个以上主流LLM提供商，各提供商API接口规范、调用协议、计费模式差异显著。企业缺乏统一的模型接入层和系统化的性能评估工具，难以科学地进行模型选型决策。
2. **异构接口集成难度**：不同提供商的API采用了不同的认证机制（OAuth 2.0、API Key、JWT等）、请求格式（OpenAI格式、Anthropic格式等）和响应协议（同步响应、Server-Sent Events流式响应等）。企业需要针对每个提供商开发独立的适配器代码，开发和维护成本居高不下。
3. **模型微调技术门槛**：通用大语言模型在特定领域的表现受限于预训练数据分布，企业需要通过参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）技术如LoRA[7]、QLoRA[8]等方法进行领域适配。但微调过程涉及数据预处理、超参数调优、分布式训练等复杂环节，对企业技术团队的要求较高。
4. **训练过程可观测性不足**：大语言模型微调任务通常需要数小时至数天的训练时间，且涉及损失函数、梯度范数、学习率衰减等多维度指标。缺乏实时监控和可视化分析工具，导致训练过程不透明，异常情况难以及时发现和处理。
5. **知识库集成与检索增强需求**：企业业务场景往往需要结合私有知识库进行检索增强生成（Retrieval-Augmented Generation, RAG），但缺乏将向量数据库、知识库管理与LLM推理统一集成的平台化解决方案。

针对上述技术挑战，本研究设计并实现了一个基于微服务架构的企业级大语言模型训练管理平台。系统采用FastAPI框架构建后端服务，Vue 3框架构建前端界面，实现了多LLM提供商统一接入、参数高效微调、实时训练监控、RAG知识库集成等核心功能，为企业提供了大语言模型应用全生命周期的完整管理方案。

系统主要功能包括：（1）基于JWT RFC 7519[13]的双令牌认证机制，Access Token有效期15分钟，Refresh Token有效期7天；（2）统一LLM客户端抽象层，通过适配器模式封装OpenAI、Anthropic、DeepSeek等15+提供商API；（3）基于Server-Sent Events的流式响应机制，实现思维链<think>标签实时解析；（4）集成LLaMA-Factory[27]训练框架，支持LoRA（r=8, alpha=16）和QLoRA微调；（5）集成SwanLab可视化平台，实时监控训练损失、学习率等指标；（6）基于Qdrant向量数据库的RAG检索增强，集成Dify知识库管理平台。

系统采用SQLAlchemy 2.x ORM进行数据库抽象，通过Alembic实现版本化迁移。前端采用Vue 3 Composition API和TypeScript类型系统，使用Vuex进行状态管理，Element Plus组件库构建用户界面。经过完整的单元测试（覆盖率86.9%）、集成测试和性能测试，系统在200并发用户场景下API平均响应时间为280ms，流式响应首Token延迟为320ms，满足企业级应用性能要求。

本文的创新点在于：（1）设计了基于策略模式的统一LLM客户端抽象层，实现了对异构模型提供商API的无缝集成；（2）实现了SSE流式响应与思维链实时解析的结合，提升了模型推理过程的可解释性；（3）构建了集成LLaMA-Factory和SwanLab的完整模型训练监控工作流；（4）实现了基于Qdrant和Dify的企业级RAG知识库解决方案。

关键词：大语言模型；Vue 3；FastAPI；参数高效微调；检索增强生成；流式响应；JWT认证

================================================================================
一、 综述
================================================================================

（一）系统建设背景

近年来，随着人工智能技术的快速发展，大语言模型技术取得了突破性进展。2022年底ChatGPT的发布标志着AI技术进入了新的发展阶段，越来越多的企业开始探索将大语言模型应用到实际业务中。目前市场上出现了GPT-4、Claude、DeepSeek、通义千问、智谱GLM等多种大语言模型服务，这些模型在自然语言理解、文本生成、智能问答等方面展现出强大的能力。

然而，企业在实际应用大语言模型时面临诸多困难。首先，不同提供商的API接口标准不统一，企业需要针对每个提供商单独开发对接代码，开发和维护成本高；其次，通用大语言模型在特定行业和业务场景下效果有限，需要进行模型微调，但缺乏便捷的训练工具和可视化监控手段；第三，企业内部的私有知识库难以与大模型有效结合，影响了模型回答的准确性和实用性。

为解决上述问题，本项目设计并实现了一个企业级大语言模型训练管理平台。该平台基于Vue 3和FastAPI技术，为企业提供了统一的模型配置管理、对话测试、模型训练和监控等功能，帮助企业更高效地应用大语言模型技术。

（二）研究意义

本系统的开发对企业应用大语言模型具有重要的实践意义。

首先，系统通过提供统一的Web管理界面，降低了大语言模型的使用门槛。企业用户无需深入了解各个提供商的API技术细节，通过可视化的配置界面就能完成模型的接入和使用，使得更多业务人员也能够使用AI技术，提高了技术普及率。

其次，系统提供了统一的模型接入层，避免了针对每个提供商重复开发的问题。企业只需要在平台上进行简单配置，就能快速接入新的模型服务商，大大提高了开发效率，降低了系统维护的复杂度和成本。

第三，系统提供了多模型对比测试功能，企业可以使用相同的测试数据对不同模型进行效果对比，通过实际测试结果选择最适合的模型，避免了盲目选型造成的资源浪费。同时系统记录了每次测试的详细数据，方便后续分析和决策。

第四，系统集成了模型训练和可视化监控工具，为企业提供了完整的模型微调方案。企业可以利用自己的业务数据对通用模型进行定制化训练，使模型更好地适应特定的业务场景，提高模型的实用性。

最后，系统支持本地化部署，企业可以将系统部署在内网环境中，保证了数据的安全性和隐私性。这对于金融、医疗等对数据安全要求较高的行业尤为重要。

（三）术语定义

系统：大语言模型训练管理平台。
子系统：按照业务功能进行模块分类的统称。
LLM：Large Language Model，大语言模型的简称。
LoRA：Low-Rank Adaptation，低秩适配微调技术。
RAG：Retrieval-Augmented Generation，检索增强生成技术。
JWT：JSON Web Token，用于身份认证的令牌标准。

（四）技术选型

本系统采用前后端分离的架构模式，各技术组件的选型说明如下：

1. 后端技术
Python是一种广泛应用于Web开发、数据分析、人工智能等领域的高级编程语言。Python由Guido van Rossum于1991年创建，现已成为全球最流行的编程语言之一。Python具有语法简洁、易于学习、功能强大等特点，拥有丰富的第三方库和活跃的开发社区，是目前AI应用开发的首选语言。
本系统后端采用FastAPI框架进行开发。FastAPI是一个现代化的Python Web框架，基于Python 3.6+的类型提示特性，支持异步编程，性能优异。FastAPI能够自动生成符合OpenAPI标准的API文档，大大方便了接口的开发和测试。系统使用SQLAlchemy作为ORM框架，实现了数据库操作的面向对象封装，提高了代码的可维护性。使用JWT（JSON Web Token）技术实现用户身份认证和权限控制，使用bcrypt算法对用户密码进行加密存储，保证了系统的安全性。

2. 前端技术
Vue.js是一个用于构建用户界面的渐进式JavaScript框架，由尤雨溪于2014年创建。Vue.js以其简洁的语法、灵活的组件化设计和高效的性能而广受欢迎。Vue 3是Vue.js的最新版本，采用了Composition API，提供了更灵活的代码组织方式和更好的TypeScript支持，大幅提升了开发体验。
本系统前端基于Vue 3框架开发，使用Element Plus作为UI组件库。Element Plus是一套基于Vue 3的桌面端组件库，提供了丰富的界面组件，包括表单、表格、对话框等，能够快速构建美观的用户界面。系统使用Vuex进行全局状态管理，Vue Router进行前端路由控制。使用Axios作为HTTP客户端，负责与后端API的数据交互。系统采用Vite作为前端构建工具，Vite基于ES模块，具有极快的冷启动速度和热更新能力，大大提高了开发效率。

3. 数据库技术
SQLite是一个轻量级的嵌入式关系型数据库，由D. Richard Hipp于2000年创建。SQLite是一个零配置的数据库引擎，整个数据库存储在单个文件中，无需独立的数据库服务器进程，非常适合开发和小型应用场景。SQLite支持标准的SQL语法，具有ACID事务特性，保证了数据的完整性和可靠性。
本系统开发环境采用SQLite作为数据库，存储用户信息、模型配置、聊天会话、训练任务等业务数据。SQLite简化了开发环境的搭建过程，降低了系统部署的复杂度。在生产环境中，系统可以通过修改配置轻松切换到MySQL或PostgreSQL等企业级数据库，以满足更高的性能和并发需求。

4. 模型训练工具
本系统集成了SwanLab作为模型训练的可视化监控工具。SwanLab是一个开源的机器学习实验跟踪平台，能够自动记录训练过程中的各项指标，如损失值、准确率、学习率等，并以图表的形式实时展示，帮助用户直观地了解模型的训练状态。系统支持与LLaMA-Factory等主流的大语言模型微调框架集成，提供完整的模型训练管理方案。

（五）系统运行环境

1. 软件环境
名称	详细要求
操作系统	Linux / Windows Server / macOS
Python环境	Python 3.10+
Node.js环境	Node.js 18.0+
数据库	MySQL 8.0+ / PostgreSQL 15.0+
Web服务器	Nginx 1.24+
GPU驱动	CUDA 11.8+ (模型训练需要)
表1-5-1 软件环境

2. 硬件环境
名称	详细要求
CPU	Intel i5以上或同等AMD处理器
内存	16GB以上
硬盘	500GB以上SSD
网卡	千兆以太网卡
GPU	NVIDIA显卡 (模型训练需要，推荐RTX 4090或A100)
显示器	支持1920x1080分辨率
表1-5-2 硬件环境

================================================================================
二、 需求分析
================================================================================

（一）系统业务总体需求

从满足企业大语言模型应用全生命周期管理的角度出发，本系统将建设一个涵盖模型配置、对话测试、模型训练、可视化监控等功能的综合性业务平台。系统重点解决企业在大模型应用中遇到的接口异构化、模型选型困难、训练门槛高、成本不透明等核心问题。

根据对企业大语言模型应用场景的调研以及各个业务部门的需求分析，通过高内聚、松耦合、模块复用的设计原则，系统细化为以下核心功能模块：

序号	功能模块	功能描述
1	用户认证与权限管理	实现用户注册、登录、角色权限控制等基础功能
2	模型配置管理	管理模型提供商信息和模型配置，实现多模型统一接入
3	模型对话测试	提供对话交互界面，支持流式响应和思维链可视化
4	模型对比测试	支持多模型并行对比测试，直观展示性能差异
5	模型训练管理	管理训练数据集和训练任务，配置训练参数
6	训练可视化监控	实时展示训练日志和监控指标，生成训练报告
7	系统提示词管理	管理系统提示词模板，支持变量注入和分类检索
8	系统管理功能	提供系统监控仪表盘和操作日志审计功能
表2-1-1 系统功能模块

（二）系统功能需求分析

1. 用户认证与权限管理
用户认证与权限管理是系统安全性的基础，负责用户身份认证和权限控制。

模块	功能点
用户认证与权限管理	用户注册
	用户登录
	密码修改
	角色权限管理
	Token刷新
表2-2-1 用户认证与权限管理

2. 模型配置管理
模型配置管理负责管理模型提供商和模型配置信息，实现多模型的统一接入。

模块	功能点
模型配置管理	提供商管理
	API配置
	模型列表同步
	模型参数预设
	连接测试
表2-2-2 模型配置管理

3. 模型对话测试
模型对话测试提供类似ChatGPT的对话交互界面，支持多轮对话和流式响应。

模块	功能点
模型对话测试	会话管理
	多轮对话
	流式响应
	Markdown渲染
	思维链可视化
	消息操作
表2-2-3 模型对话测试

4. 模型对比测试
模型对比测试支持同时向多个模型发送相同提示词，直观对比响应效果。

模块	功能点
模型对比测试	多模型选择
	批量测试
	结果对比展示
	测试记录保存
表2-2-4 模型对比测试

5. 模型训练管理
模型训练管理负责管理训练数据集和训练任务，提供可视化的训练配置界面。

模块	功能点
模型训练管理	数据集管理
	数据集校验
	任务创建
	训练参数配置
	任务状态控制
表2-2-5 模型训练管理

6. 训练可视化监控
训练可视化监控提供训练过程的实时监控和可视化展示。

模块	功能点
训练可视化监控	实时日志流
	训练进度展示
	Loss曲线
	学习率曲线
	训练报告生成
表2-2-6 训练可视化监控

7. 系统提示词管理
系统提示词管理提供系统提示词模板的集中管理功能。

模块	功能点
系统提示词管理	模板创建
	模板编辑
	变量定义
	分类管理
	模板检索
表2-2-7 系统提示词管理

8. 系统管理功能
系统管理功能提供系统运行状态监控和操作日志审计。

模块	功能点
系统管理功能	用户管理
	角色权限管理
	用户统计
表2-2-8 系统管理功能

（三）系统非功能需求分析

1. 性能需求
API接口平均响应时间应控制在300ms以内，单节点应支持至少200个并发用户同时在线操作。流式响应的首Token时间主要取决于底层模型推理速度，系统自身引入的额外延迟应控制在50ms以内。

2. 安全性需求
系统采用JWT标准进行身份认证，Access Token有效期15分钟，Refresh Token有效期7天。用户密码使用bcrypt算法加盐存储，第三方模型的API Key在数据库中加密存储。系统通过HTTPS协议传输数据，防止数据泄露。

3. 可扩展性需求
LLM客户端采用策略模式设计，新增模型提供商时只需实现统一接口，无需修改核心业务逻辑。后端服务采用无状态设计，支持通过增加服务节点实现水平扩展和负载均衡。

4. 可用性需求
前端界面简洁直观，符合Element Plus设计规范。系统提供清晰的错误提示信息，帮助用户快速定位和解决问题。系统支持Docker容器化部署，提供一键部署方案。

================================================================================
三、 系统总体设计
================================================================================

（一） 业务流程图

本系统的核心业务流程围绕大语言模型的全生命周期管理展开，涵盖模型接入配置、对话推理应用和模型微调训练三条主要业务线。

1. **模型接入配置流程**：管理员登录 -> 进入模型配置中心 -> 添加/编辑提供商信息 -> 点击“刷新列表” -> 系统自动调用提供商API获取模型列表 -> 保存可用模型配置。
2. **对话推理应用流程**
用户登录系统后进入对话界面，选择目标模型并输入提示词。系统根据模型路由策略调用对应的模型API，接收SSE（Server-Sent Events）流式响应数据，前端实时渲染模型回复内容。对于支持思维链（Chain of Thought）的模型，系统还会解析并展示推理过程。
3. **模型微调训练流程**
用户上传指令微调数据集后，系统自动进行格式校验。用户创建训练任务并配置训练参数（如LoRA秩、学习率、训练轮数等）。训练启动后，系统调度GPU计算资源执行微调任务，实时回传训练损失、学习率等监控指标至前端。训练完成后，系统自动注册新的模型版本供后续使用。

（二） 系统逻辑架构图

系统采用经典的分层架构设计模式，自下而上划分为基础设施层、数据存储层、核心服务层、接口网关层和用户交互层五个逻辑层次。各层职责明确，耦合度低，便于系统维护和扩展。

1. **用户交互层（Frontend）**：基于Vue 3 + Element Plus构建的SPA单页应用，负责页面展示、用户交互、状态管理（Vuex）和路由控制。
2. **接口网关层（API Gateway）**：基于FastAPI提供的RESTful API接口，负责路由分发、身份认证（JWT）、请求限流和CORS跨域处理。
3. **核心服务层（Backend Services）**：
   - **认证授权服务**：处理用户注册、登录、Token签发与刷新。
   - **LLM统一接入服务**：封装OpenAI、Anthropic、Ollama等异构接口，提供标准化的推理能力。
   - **训练管理服务**：集成LLaMA-Factory，管理训练任务生命周期。
   - **RAG知识服务**：管理向量索引，执行文档解析和语义检索。
   - **监控服务**：集成SwanLab，采集训练和运行指标。
4. **数据存储层（Data Persistence）**：
   - **关系型数据库（MySQL）**：存储用户、配置、会话历史、任务元数据。
   - **向量数据库（Qdrant）**：存储知识库文档的向量嵌入（Embeddings）。
   - **缓存数据库（Redis）**：缓存热点数据、Session状态、验证码。
5. **基础设施层（Infrastructure）**：包含CPU/GPU计算资源、Docker容器运行环境、Nginx反向代理服务器等。

（三） 系统结构图

系统物理部署采用基于Docker的微服务容器化部署方案，各服务组件独立部署，通过容器编排实现统一管理。

- **前端容器**：运行Nginx Web服务器，托管Vue 3编译生成的静态资源文件，并作为反向代理转发后端API请求。
- **后端容器**：运行Uvicorn ASGI服务器（生产环境配合Gunicorn进行进程管理），承载FastAPI应用程序，通过Docker内部网络访问数据库服务。
- **数据库容器组**：包含MySQL关系型数据库、Redis缓存数据库、Qdrant向量数据库三个容器，数据文件通过Docker Volume挂载实现持久化存储。
- **训练工作节点**：配置NVIDIA GPU驱动和CUDA计算环境，运行LLaMA-Factory训练进程。训练节点与后端服务通过RESTful API或消息队列进行通信，实现训练任务的调度和监控数据的回传。

（四） 技术架构设计

1. **体系结构**
系统采用B/S（Browser/Server）架构模式，遵循前后端分离的设计原则。前端负责界面渲染和用户交互逻辑，后端负责业务逻辑处理和数据持久化。两者通过HTTP/HTTPS协议进行通信，数据交换采用JSON格式。对于实时性要求较高的对话场景，系统采用SSE（Server-Sent Events）技术实现服务器向客户端的单向流式数据推送，保证用户体验的流畅性。

2. **技术总体结构模型图**
- **开发语言**：Python 3.10+ (后端/AI处理), JavaScript/TypeScript (前端)。
- **Web框架**：FastAPI (高性能异步框架)。
- **前端框架**：Vue 3 (Composition API), Vite (构建工具), Element Plus (UI组件库)。
- **ORM框架**：SQLAlchemy 2.0 (异步支持)。
- **AI/LLM框架**：LLaMA-Factory (微调), LangChain/LlamaIndex (RAG思路参考), HuggingFace Transformers。
- **可视化工具**：SwanLab (训练监控)。

（五） 数据库设计

1. **概念模型（E-R图）**
系统核心实体包括：用户（User）、模型提供商（Provider）、模型配置（ModelConfig）、会话（Session）、消息（Message）、训练任务（TrainingJob）、数据集（Dataset）等七个主要实体。实体间关系如下：
- **用户**与**会话**是一对多关系。
- **会话**与**消息**是一对多关系。
- **模型提供商**与**模型配置**是一对多关系。
- **训练任务**关联一个**基座模型**和一个**数据集**。

2. **逻辑模型设计**
数据库设计严格遵循第三范式（3NF）规范，确保数据冗余最小化和数据一致性。所有表均包含`created_at`（创建时间）和`updated_at`（更新时间）时间戳字段，便于数据审计和版本追踪。主键根据业务场景选用自增整型ID或UUID，保证数据唯一性。

3. **核心业务表设计**

（1）**用户表 (users)**
| 字段名 | 类型 | 说明 |
| :--- | :--- | :--- |
| id | Integer | 主键，自增 |
| email | String(255) | 邮箱，唯一索引 |
| hashed_password | String | bcrypt哈希后的密码 |
| is_active | Boolean | 账户状态 |
| role | String | 角色（admin/user） |

（2）**模型配置表 (model_configs)**
| 字段名 | 类型 | 说明 |
| :--- | :--- | :--- |
| id | Integer | 主键 |
| provider_id | String | 提供商标识（openai/deepseek等） |
| name | String | 模型显示名称 |
| model_id | String | 实际调用的模型ID |
| api_key | String | 加密存储的API Key |
| base_url | String | API基础地址 |
| is_enabled | Boolean | 启用状态 |

（3）**聊天会话表 (chat_sessions)**
| 字段名 | 类型 | 说明 |
| :--- | :--- | :--- |
| id | String(36) | UUID主键 |
| user_id | Integer | 关联用户ID |
| model_config_id | Integer | 使用的模型配置ID |
| title | String | 会话标题 |

（4）**聊天消息表 (chat_messages)**
| 字段名 | 类型 | 说明 |
| :--- | :--- | :--- |
| id | Integer | 主键 |
| session_id | String(36) | 关联会话ID |
| role | String | 角色（user/assistant/system） |
| content | Text | 消息内容 |
| thinking_content | Text | 思维链内容（如DeepSeek R1的思考过程） |
| meta_info | JSON | 元数据（token消耗、耗时等） |

（5）**训练任务表 (training_jobs)**
| 字段名 | 类型 | 说明 |
| :--- | :--- | :--- |
| id | String(36) | UUID主键 |
| status | String | 任务状态（pending/running/success/failed） |
| base_model | String | 基座模型路径 |
| dataset_path | String | 数据集路径 |
| hyperparameters | JSON | 训练超参数（lr, epochs, lora_rank等） |
| swanlab_run_id | String | SwanLab关联ID |

================================================================================
四、 系统详细设计与实现
================================================================================

本章将详细阐述系统的核心功能模块的设计与实现过程，包括后端业务逻辑实现和前端页面交互实现。系统采用前后端分离架构，后端基于FastAPI提供RESTful接口，前端基于Vue 3构建交互界面。

（一） 后端系统设计与实现

1. 用户认证与权限管理模块

用户认证是系统安全的第一道防线。本系统采用JWT（JSON Web Token）双令牌机制（Access Token + Refresh Token）实现无状态认证。

(1) 登录功能实现
用户提交邮箱和密码后，后端验证凭据。验证通过后，生成短效Access Token（15分钟）和长效Refresh Token（7天）。Access Token返回给前端用于API调用，Refresh Token写入HttpOnly Cookie用于自动续期。

(2) JWT双令牌认证机制
后端使用`python-jose`库生成和验证Token。

核心代码实现（`backend/app/api/auth.py`）：

```python
@router.post("/login", response_model=Token)
async def login_for_access_token(
    form_data: OAuth2PasswordRequestForm = Depends(),
    db: Session = Depends(get_db)
):
    # 验证用户凭据
    user = authenticate_user(db, form_data.username, form_data.password)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # 生成Access Token
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user.email}, expires_delta=access_token_expires
    )
    
    # 生成Refresh Token
    refresh_token_expires = timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    refresh_token = create_refresh_token(
        data={"sub": user.email}, expires_delta=refresh_token_expires
    )
    
    # 设置Refresh Token Cookie
    response = JSONResponse(content={
        "access_token": access_token, 
        "token_type": "bearer",
        "user": {"email": user.email, "role": user.role}
    })
    response.set_cookie(
        key="refresh_token",
        value=refresh_token,
        httponly=True,  # 防止XSS攻击
        secure=True,    # 仅通过HTTPS传输
        samesite="lax"
    )
    return response
```

2. 模型配置管理模块

该模块负责管理不同LLM提供商的连接信息。

(1) 模型提供商配置
支持添加OpenAI、DeepSeek、Anthropic等厂商配置。密码/Key字段在入库前通过Fernet加密存储。

(2) 模型列表刷新机制
为了解决手动输入模型ID容易出错的问题，系统实现了自动从提供商API获取模型列表的功能。

核心代码实现（`backend/app/api/model_config.py`）：

```python
@router.post("/{config_id}/refresh-models")
async def refresh_models(config_id: int, db: Session = Depends(get_db)):
    config = db.query(ModelConfig).filter(ModelConfig.id == config_id).first()
    if not config:
        raise HTTPException(status_code=404, detail="配置不存在")
        
    try:
        # 实例化对应的LLM客户端
        client = LLMClientFactory.create_client(config.provider_type, config.api_key, config.base_url)
        # 调用远程API获取模型列表
        remote_models = await client.list_models()
        
        # 更新本地数据库
        config.available_models = json.dumps(remote_models)
        config.last_refreshed = datetime.now()
        db.commit()
        return {"status": "success", "models": remote_models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

3. LLM统一调用模块

为了屏蔽不同厂商API的差异（如OpenAI接口与Anthropic接口格式不同），系统设计了统一的LLM客户端抽象层。

(1) 基础客户端抽象设计
所有具体的LLM客户端都继承自`BaseLLMClient`，实现`chat_completion`和`stream_chat_completion`方法。

核心代码实现（`backend/app/llm_core/base.py`）：

```python
class BaseLLMClient(ABC):
    @abstractmethod
    async def chat_completion(self, messages: list, model: str, **kwargs) -> str:
        pass

    @abstractmethod
    async def stream_chat_completion(self, messages: list, model: str, **kwargs):
        pass
```

(2) 流式响应处理机制
通过Python的`async generator`实现流式数据传输，支持Server-Sent Events (SSE) 协议。

4. 模型对话模块

(1) 会话管理功能
将用户的每轮对话组织成Session，支持新建、查询、删除会话。

(2) 流式输出与SSE实现
前端通过EventSource连接后端SSE接口，后端实时推送模型生成的Token。

核心代码实现（`backend/app/api/chat.py`）：

```python
@router.post("/chat/stream")
async def stream_chat(
    request: ChatRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    async def event_generator():
        # 获取模型客户端
        client = get_llm_client(request.model_config_id, db)
        
        # 调用流式接口
        stream = await client.stream_chat_completion(
            messages=request.messages,
            model=request.model_id
        )
        
        async for chunk in stream:
            # 封装为SSE格式
            yield f"data: {json.dumps({'content': chunk})}\n\n"
        
        yield "data: [DONE]\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

(3) 思维链(<think>)解析展示
对于DeepSeek R1等推理模型，后端会识别输出中的`<think>`标签，将其内容单独标记，供前端渲染为折叠的思考过程。

5. 训练任务管理模块

集成LLaMA-Factory框架，实现微调任务的启动与管理。

核心代码实现（`backend/app/api/training.py`）：

```python
@router.post("/jobs")
async def create_training_job(
    job_create: TrainingJobCreate,
    db: Session = Depends(get_db)
):
    # 构建LLaMA-Factory启动命令
    cmd = [
        "llamafactory-cli", "train",
        "--stage", "sft",
        "--model_name_or_path", job_create.base_model,
        "--dataset", job_create.dataset,
        "--output_dir", f"./outputs/{job_create.job_name}",
        "--finetuning_type", "lora",
        "--lora_rank", str(job_create.lora_rank),
        "--per_device_train_batch_size", str(job_create.batch_size),
        "--learning_rate", str(job_create.learning_rate)
    ]
    
    # 异步启动子进程
    process = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )
    
    # 记录任务ID
    new_job = TrainingJob(
        id=str(uuid.uuid4()),
        pid=process.pid,
        status="running",
        config=job_create.dict()
    )
    db.add(new_job)
    db.commit()
    
    return new_job
```

（二） 前端系统设计与实现

1. 前端技术架构与项目结构

前端采用Vue 3 + Vite构建，使用Composition API组织逻辑，Store (Vuex) 管理全局状态。

主要目录结构：
- `src/api`：封装Axios请求
- `src/views`：页面组件
- `src/components`：公共组件
- `src/store`：状态管理
- `src/utils`：工具函数（如SSE解析器）

2. 登录注册页面设计与实现

(1) 登录页面UI设计
使用Element Plus的`el-form`组件构建表单，支持表单验证规则（如邮箱格式、密码长度）。

(2) JWT Token存储机制
登录成功后，将后端返回的Access Token存储在localStorage中，并在后续Axios请求拦截器中自动添加到Header。

前端代码片段（`frontend/src/utils/request.js`）：

```javascript
service.interceptors.request.use(
  config => {
    const token = localStorage.getItem('access_token')
    if (token) {
      config.headers['Authorization'] = 'Bearer ' + token
    }
    return config
  },
  error => {
    return Promise.reject(error)
  }
)

service.interceptors.response.use(
  response => response.data,
  async error => {
    // 401错误尝试刷新Token
    if (error.response.status === 401 && !originalRequest._retry) {
        // 调用刷新Token接口...
    }
