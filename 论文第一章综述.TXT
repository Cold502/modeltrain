
================================================================================
一、综述
================================================================================

（一）系统建设背景

近年来，以GPT-5、Claude 4、LLaMA 4、Qwen 3为代表的大语言模型[1]（Large Language Model，简称LLM）取得了突破性进展，展现出强大的自然语言理解和生成能力。特别是2025年发布的DeepSeek-V3和DeepSeek-R1，在推理能力和性价比方面实现了重大突破。这些模型在文本生成、对话交互、代码编写、知识问答、复杂推理等多个领域都表现出接近或超越人类的水平。企业纷纷开始探索将大语言模型应用于客户服务、内容创作、数据分析、智能助手等业务场景，以期提升运营效率和服务质量。

然而，在企业实际应用大语言模型的过程中，面临着诸多挑战：

首先，模型提供商众多且接口不统一。市场上既有OpenAI、Anthropic（Claude）、DeepSeek、智谱AI等提供云端API服务的商业厂商，也有Ollama、vLLM、SGLang、LMDeploy等支持本地部署的开源方案。不同提供商的API接口格式、参数配置、调用方式存在差异，企业技术人员需要针对每个平台单独编写和维护接入代码，开发成本高昂。同时，企业在模型选型时缺乏统一的测试平台，难以快速对比评估不同模型在具体业务场景下的表现。

其次，模型配置和管理复杂。每个LLM提供商都需要配置API端点、密钥、模型名称等参数，还涉及temperature、max_tokens、top_p等多个生成参数的调优。企业往往需要在多个平台的管理后台之间频繁切换，配置信息分散存储，缺乏统一的配置管理和版本控制机制。当需要更换模型或调整参数时，修改和回滚都较为困难。

第三，通用模型难以满足专业需求。虽然通用大语言模型在广泛的任务上表现良好，但在特定行业或专业领域（如医疗、法律、金融等），其回答的准确性和专业性往往不足。企业需要通过模型微调（Fine-tuning）来适配自己的业务场景。然而，模型微调涉及数据准备、训练参数配置、GPU资源管理、训练过程监控等一系列技术环节，对于非AI专业的企业技术团队来说门槛较高。

第四，缺乏知识积累和复用机制。在日常使用大语言模型的过程中，企业会积累大量有效的提示词（Prompt）模板和对话记录。然而，这些宝贵的知识资产往往分散在个人电脑或聊天记录中，缺乏系统化的管理和团队共享机制，导致知识无法沉淀和复用。

第五，可视化监控缺失。模型训练过程通常需要数小时甚至数天，训练期间的损失曲线、学习率变化、GPU使用率等关键指标需要实时监控。传统的命令行日志方式不够直观，企业缺乏友好的可视化工具来跟踪训练进度和分析训练效果，导致训练过程不透明，出现问题难以及时发现和调整。

基于以上背景，开发一个统一的企业模型训练管理平台具有重要的现实意义。本系统旨在为企业提供一个集模型配置、对话测试、效果对比、模型训练、可视化监控于一体的综合性管理平台，降低大语言模型的使用门槛，提升企业在AI应用方面的效率和能力。

（二）研究意义

本系统的研发具有以下重要意义：

1. 降低技术门槛

通过统一的Web界面，企业用户无需编写代码即可配置和使用多个大语言模型。系统屏蔽了不同LLM提供商API接口的差异，提供了一致的操作体验。技术人员只需在系统中填写配置信息，即可快速接入新的模型。对于业务人员，也可以通过简洁的对话界面直接使用各类模型，无需了解底层技术细节。这显著降低了大语言模型的使用门槛，使更多非技术背景的员工也能够利用AI技术提升工作效率。

2. 提高工作效率

系统提供了模型配置管理、批量测试、历史记录导出、提示词模板管理等一系列功能，大幅提升了企业在模型选型和应用过程中的工作效率。例如，在模型选型阶段，技术人员可以使用批量测试功能，一次性对多个模型进行同样的问题测试，快速对比各模型的回答质量和响应速度，科学地选择最适合的模型。在日常使用中，通过提示词模板功能，可以将常用的任务提示词保存为模板，一键应用，避免重复编写。

3. 节约成本

通过多模型对比测试功能，企业可以科学地评估不同模型的性价比。在保证效果的前提下，选择成本更低的模型或提供商，避免不必要的API调用费用。同时，系统支持本地部署的Ollama、vLLM、SGLang等开源推理引擎，企业可以自建推理服务，使用开源模型替代部分商业API，进一步降低长期运营成本。此外，统一的管理平台减少了开发和维护多套接入代码的成本，节约了人力资源。

4. 支持模型微调

系统集成了LLaMA-Factory这一流行的开源微调框架，为企业提供了便捷的模型微调能力。LLaMA-Factory支持100+种预训练模型和多种微调方法（LoRA、QLoRA等），企业可以根据自己的业务数据对通用模型进行微调，使其更好地适应特定场景。系统提供了简化的配置界面和训练任务管理功能，降低了模型微调的技术门槛。通过微调，企业可以在开源模型的基础上训练出专属的领域模型，在保障数据隐私的同时获得更好的业务效果。

5. 增强可控性

系统支持本地部署的Ollama、vLLM、SGLang等推理引擎，企业可以在内网环境中部署和使用大语言模型。这对于处理敏感数据、有数据合规要求或网络隔离需求的企业尤为重要。本地部署方案不仅保障了数据安全，也使企业不再依赖外部API服务的可用性，增强了业务连续性。同时，企业对本地模型拥有完全的控制权，可以根据需要定制和优化模型行为。

6. 促进知识积累

通过系统提示词管理功能，企业可以积累和复用优秀的提示词模板，形成企业级的知识资产。有效的提示词往往是经过反复测试和优化的，代表了企业在特定任务上的最佳实践。将这些提示词模板化并集中管理，可以方便团队成员共享和使用，避免重复试错，提升整体工作质量。此外，对话历史记录的导出和分析功能，也有助于企业发现模型应用的规律和优化空间。

综上所述，本系统的建设不仅解决了企业在使用大语言模型过程中的实际痛点，也为企业构建了一个可持续发展的AI应用基础设施，具有重要的实用价值和推广意义。

（三）术语定义

为便于理解，对本文涉及的核心术语进行定义：

1. LLM（Large Language Model，大语言模型）

大语言模型是指参数规模达到数十亿甚至数千亿的深度学习模型[2][3]。这类模型基于Transformer架构[4]，通过在海量文本数据（通常包含数万亿Token）上进行预训练，学习到丰富的语言知识和世界知识，从而具备强大的自然语言理解和生成能力。代表性的大语言模型包括OpenAI的GPT-5和o1系列、Anthropic的Claude 4系列、Meta的LLaMA 4系列[5][6]、阿里的Qwen 3系列、DeepSeek的V3和R1系列、清华智谱的GLM-5系列等。这些模型的发展经历了从词向量表示[7][8]、上下文化词表示[9]到大规模预训练语言模型[10][11][12]的演进过程。截至2026年初，这些模型能够完成问答、翻译、摘要、代码生成、复杂推理、多模态理解等多种自然语言处理任务。

2. Fine-tuning（微调）

微调是指在预训练模型的基础上，使用特定领域或任务的数据进行进一步训练，使模型更好地适应特定应用场景的过程。预训练模型虽然具备通用能力，但在专业领域可能表现不佳。通过微调，模型可以学习到领域特定的知识和表达方式。微调过程中通常采用Adam[13]等优化算法，并使用Dropout[14]、Batch Normalization[15]、Layer Normalization[16]等技术防止过拟合和加速训练。模型微调技术借鉴了迁移学习[17]的思想，通过在预训练模型基础上进行特定任务的训练来提升性能。为降低计算和存储成本，还可采用知识蒸馏[18]和模型量化[19]等技术。微调通常只需要少量的训练数据和计算资源，就能显著提升模型在特定任务上的性能。常见的微调方法包括全参数微调和参数高效微调（如LoRA[20]、QLoRA[21]）。

3. SSE（Server-Sent Events，服务器推送事件）

SSE是一种服务器向客户端推送实时数据的技术标准。与传统的HTTP请求-响应模式不同，SSE允许服务器在单个HTTP连接上持续向客户端发送数据流。本系统使用SSE技术实现大语言模型的流式响应输出，即模型生成的文本逐字逐句地实时传输到前端显示，而不是等待全部生成完毕后再返回，从而提供了更好的用户体验。SSE相比WebSocket更加轻量，适合单向的服务器到客户端推送场景。

4. 思维链（Chain of Thought，CoT）

思维链是指大语言模型在回答问题时展示的推理过程。一些先进的推理模型（如DeepSeek-R1、OpenAI的o1和o3系列、Claude 4 Reasoning系列）在生成最终答案之前，会先输出中间的推理步骤，这些步骤通常用特殊标签（如`<think>`）包裹。思维链技术在2025年得到了广泛应用，显著提升了模型在数学、编程、逻辑推理等复杂任务上的表现。本系统能够识别和解析思维链标签，在界面上以特殊样式展示推理过程，使用户能够看到模型"是如何思考的"。

5. Prompt（提示词）

提示词是用户或系统提供给大语言模型的输入文本，用于引导模型生成期望的输出。提示词的质量直接影响模型的输出效果。一个好的提示词通常包含明确的任务描述、必要的背景信息、期望的输出格式等。通过精心设计提示词[22]（Prompt Engineering），可以显著提升模型在特定任务上的表现，而无需对模型进行微调。指令微调[23]技术进一步增强了模型遵循指令的能力。本系统提供提示词模板管理功能，帮助用户积累和复用有效的提示词。

6. JWT（JSON Web Token）

JWT是一种基于JSON的开放标准[24]（RFC 7519），用于在网络应用环境间安全地传递信息。JWT令牌由三部分组成：头部（Header）、载荷（Payload）和签名（Signature）。本系统使用JWT进行用户身份认证，采用Access Token和Refresh Token的双令牌机制。Access Token有效期较短，用于API请求认证；Refresh Token有效期较长，用于刷新Access Token。这种机制在保证安全性的同时，提供了良好的用户体验。

7. ORM（Object-Relational Mapping，对象关系映射）

ORM是一种将数据库表结构映射为程序对象的技术，使开发者可以使用面向对象的方式操作数据库，而无需编写SQL语句。ORM框架自动处理对象和数据库记录之间的转换，简化了数据库操作代码，提高了开发效率。本系统使用SQLAlchemy作为ORM框架。SQLAlchemy是Python生态中最流行、功能最强大的ORM工具，支持多种数据库，提供了丰富的查询接口和事务管理功能。

8. LLaMA-Factory

LLaMA-Factory是一个易于使用的大语言模型微调框架，由开源社区开发和维护。它支持多种主流模型（LLaMA、Qwen、GLM、Mistral、Gemma等100+种）和多种微调方法（LoRA、QLoRA、全参数微调、DoRA、AdaLoRA等）。LLaMA-Factory提供了WebUI（基于Gradio）和命令行两种使用方式，大幅降低了模型微调的技术门槛。

LLaMA-Factory的主要特点包括：

（1）广泛的模型支持：支持LLaMA-4、Qwen-3、GLM-5、Mistral-Large、DeepSeek-V3、Gemma-3等150+种预训练模型，涵盖2026年初的主流开源和商业模型。企业可以根据需求选择合适的基础模型进行微调。

（2）多样的微调方法：集成LoRA（低秩适配）、QLoRA（量化LoRA）、全参数微调、DoRA等多种算法，适应不同的资源和性能需求。这些方法的选择可以借鉴神经架构搜索[25]的思想，根据具体任务和资源约束自动选择最优配置。LoRA和QLoRA特别适合资源受限的场景，在消费级GPU上也能微调大型模型。

（3）灵活的数据处理：支持Alpaca、ShareGPT、OpenAI等多种数据格式，内置常用数据集，也支持自定义数据。数据预处理pipeline经过充分优化，可高效处理大规模训练数据。

（4）训练监控集成：原生支持SwanLab、TensorBoard、Weights & Biases等可视化工具，方便实时监控训练过程和对比不同实验。

（5）推理优化支持：支持vLLM、llama.cpp等高性能推理引擎，训练后的模型可以高效部署到生产环境。

（6）易用性强：提供友好的WebUI界面和详细的命令行工具，配备完善的中英文文档和活跃的社区支持。即使是非AI专业背景的开发者，也能快速上手使用。

本系统将LLaMA-Factory集成作为训练引擎，为企业用户提供便捷的模型微调能力。

9. SwanLab

SwanLab是一个开源的机器学习实验跟踪与可视化工具，专为深度学习训练过程设计。它可以自动记录训练指标、可视化损失曲线、对比不同实验结果，并提供远程监控功能。SwanLab被设计为Weights & Biases的开源替代方案，既可以部署在本地，也可以使用云端服务。

SwanLab的主要特性包括：

（1）自动记录训练指标：包括损失（loss）、准确率（accuracy）、学习率（learning rate）、梯度范数等各类训练指标，无需手动编写记录代码。

（2）实时可视化训练曲线：提供美观的图表展示训练过程，支持多种图表类型（折线图、直方图、散点图等），可以直观地观察模型收敛情况。

（3）多实验对比分析：可以在同一界面中对比不同超参数配置下的训练结果，快速找到最优配置。支持并排展示多个实验的指标曲线。

（4）本地和云端部署：既支持完全离线的本地部署（适合数据隐私要求高的场景），也支持云端托管（方便远程团队协作），满足不同的使用需求。

（5）框架无缝集成：与PyTorch、TensorFlow、LLaMA-Factory等主流训练框架无缝集成，通过简单的回调函数或几行代码即可启用。

（6）swanlab watch命令：可以监控已有的训练日志目录，即使训练已经开始或结束，也能通过此命令启动可视化服务查看历史记录。这对于分析历史训练任务非常方便。

本系统集成SwanLab，为模型训练提供强大的可视化监控能力。LLaMA-Factory训练时会自动记录指标到SwanLab，本系统通过swanlab watch命令启动可视化服务，用户可以在浏览器中实时查看训练进度和效果。

（四）技术选型

本系统在技术选型时遵循成熟稳定、性能优异、社区活跃的原则，选择了一套现代化的技术栈，以确保系统的可靠性、可维护性和扩展性。

1. 后端技术栈

（1）FastAPI框架
系统后端采用现代化的Web框架，基于Python 3.7+构建RESTful API[26]。相比传统的Flask和Django框架，FastAPI在性能、开发效率和易用性方面都有显著优势。

主要特点：
- 高性能：基于Starlette和Pydantic构建，性能接近NodeJS和Go，在同类Python框架中名列前茅
- 自动生成API文档：基于OpenAPI标准自动生成交互式API文档（Swagger UI和ReDoc），极大方便了前后端联调
- 原生异步支持：原生支持async/await，适合处理大量并发请求。本系统的流式响应和LLM API调用都采用了异步处理
- 自动数据验证：基于Python类型提示和Pydantic模型进行自动数据验证和序列化，减少了手动校验代码，提升了代码质量
- 类型提示支持：充分利用Python的类型提示，提供更好的IDE补全和错误检查

选择FastAPI作为后端框架，不仅满足了系统对高性能和高并发的需求，其现代化的设计理念也大幅提升了开发效率。

（2）SQLAlchemy 2.x
系统采用对象关系映射（ORM）技术来管理数据库操作，提供统一的数据访问接口。本系统采用2.x版本，该版本相比1.x带来了显著的性能提升和更现代的API设计。

主要优势：
- 强大的ORM功能：提供丰富的查询构建器和关系映射能力，支持复杂的数据库操作
- 多数据库支持：统一的API接口支持SQLite、MySQL、PostgreSQL等多种数据库，切换数据库时业务代码无需修改
- 异步支持：2.x版本引入了完整的异步API支持，与FastAPI的异步特性完美配合
- 事务管理：提供完善的事务管理和连接池机制，保证数据操作的ACID特性

SQLAlchemy的数据库抽象能力使得本系统可以轻松适配不同企业的数据库环境，具有良好的可移植性。

（3）Alembic
Alembic是SQLAlchemy官方推荐的数据库迁移工具，与SQLAlchemy无缝集成。在软件迭代过程中，数据库结构的变更是常态。Alembic提供了版本控制、自动生成迁移脚本、前滚和回滚等功能，确保数据库结构变更的可追溯性和可靠性。

主要功能：
- 版本控制：为每次数据库变更生成唯一的版本标识，记录完整的变更历史
- 自动生成迁移脚本：通过对比模型定义和当前数据库结构，自动生成迁移脚本
- 前滚和回滚：支持将数据库升级到任意版本或回滚到历史版本
- 团队协作：迁移脚本可纳入版本控制系统，保证团队成员间数据库结构的一致性

（4）httpx
httpx是一个现代化的HTTP客户端库，被设计为requests库的异步版本。本系统使用httpx调用各个LLM提供商的API。

相比传统的requests库，httpx的优势在于：
- 异步请求支持：原生支持async/await，可以与FastAPI的异步视图函数配合，实现真正的并发处理
- HTTP/2支持：支持HTTP/2协议，可以复用连接、多路复用请求，提升网络传输效率
- 流式响应处理：提供便捷的流式响应处理接口，适合处理大语言模型的流式输出
- 现代API设计：接口设计更加直观和Pythonic，易于使用和维护

（5）LLaMA-Factory
本系统集成LLaMA-Factory[27]作为模型训练引擎。该框架支持多种主流预训练模型的微调，包括基于注意力机制[28]和序列到序列架构[29]的模型。LLaMA-Factory是当前最流行的开源LLM微调框架之一，在GitHub上拥有数万Star，得到了广泛的认可。

选择LLaMA-Factory的理由：
- 广泛的模型支持：支持100+种预训练模型，涵盖几乎所有主流的开源大语言模型
- 多样的微调方法：支持LoRA、QLoRA、全参数微调等多种方法，适应不同的资源和性能需求
- 灵活的数据处理：支持多种主流数据格式，内置常用数据集，也支持自定义数据
- 训练监控集成：原生支持SwanLab、TensorBoard等可视化工具
- 易用性强：提供WebUI和命令行两种接口，配备完善的文档和活跃的社区

（6）python-jose
python-jose是一个Python的JOSE实现，用于JWT令牌的生成和验证。本系统使用python-jose实现基于JWT的用户认证机制，提供完整的加密和签名功能，保障系统认证安全。

2. 前端技术栈

（1）Vue 3框架
前端采用现代化的组件式架构设计，通过模块化的方式构建用户界面。本系统采用Vue 3版本，相比Vue 2带来了重大改进：

- Composition API：提供更灵活的代码组织方式，通过setup函数和组合式函数，可以按功能逻辑组织代码，提升代码的可读性和可维护性
- TypeScript支持：Vue 3从头设计时就考虑了TypeScript支持，提供完整的类型定义
- 性能提升：重写了虚拟DOM算法，编译器进行了优化，组件初始化速度更快，渲染效率更高
- Tree-shaking支持：核心库支持Tree-shaking，未使用的功能模块不会被打包，减小了打包体积
- 新特性：引入了Teleport、Suspense、Fragments等新特性，扩展了框架的能力

（2）Element Plus
系统采用成熟的UI组件库，提供丰富的界面元素和交互功能。它提供了60+个UI组件，包括表格、表单、对话框、消息提示等，显著提升开发效率。

优势：
- 组件丰富：涵盖企业级应用开发所需的绝大多数UI组件
- 设计精良：所有组件都经过精心设计，界面美观大方
- 主题定制：支持主题变量定制，可以方便地适配企业的视觉风格
- 文档完善：提供详细的中英文文档和丰富的示例代码
- 社区活跃：拥有庞大的用户群体和活跃的社区

（3）Vuex
Vuex是Vue官方的状态管理库，用于管理应用的全局状态。本系统使用Vuex管理用户登录状态、聊天会话列表、暗色模式等全局状态。通过集中式存储和管理，确保状态的一致性和可预测性。

（4）Vue Router
Vue Router是Vue官方的路由管理器，实现单页应用（SPA）的页面导航。主要功能包括声明式路由配置、路由守卫（本系统利用路由守卫实现登录认证）、懒加载、动态路由等。

（5）Axios
Axios是一个基于Promise的HTTP客户端。本系统对Axios进行了封装，实现了统一的baseURL配置、请求拦截（自动添加JWT Token）、响应拦截（401错误自动刷新Token）、withCredentials配置等功能。

（6）Marked.js
Marked.js是一个快速、轻量的Markdown解析库，用于将模型返回的Markdown格式文本转换为HTML。结合highlight.js，本系统还实现了代码块的语法高亮显示。

（7）ECharts
ECharts是百度开源的企业级可视化图表库。本系统使用ECharts展示训练数据分析、系统统计等图表。ECharts支持折线图、柱状图、饼图等几十种图表类型，提供丰富的交互功能，性能优异。

（8）Vite
Vite是新一代前端构建工具，由Vue作者尤雨溪开发。相比传统的Webpack，Vite利用浏览器原生的ES模块支持，提供了更快的冷启动速度和热更新性能。

主要优势：
- 极速的开发服务器启动：无需打包，秒级启动
- 快速的热更新（HMR）：模块级别的热更新，修改代码后几乎实时反映
- 优化的生产构建：基于Rollup构建，产物体积小
- 开箱即用：内置对TypeScript、JSX、CSS预处理器等的支持

3. 数据库设计

本系统采用SQLite作为默认数据库，主要基于以下考虑：

（1）轻量级
SQLite是一个嵌入式数据库引擎，不需要独立的数据库服务器进程。整个数据库就是一个文件，可以直接放在应用程序目录中。这种架构极大简化了部署流程，特别适合中小型企业应用。用户下载系统后，无需安装配置数据库服务器，即可直接运行。

（2）零配置
SQLite是零配置的数据库，不需要进行任何安装和设置。Python标准库内置了sqlite3模块，开箱即用。这降低了系统部署的复杂度，减少了出错的可能性。

（3）可靠性
SQLite虽然轻量，但可靠性毫不逊色。它经过了极其严格的测试（测试代码量是源代码的数百倍），被广泛应用于各类软件中，包括智能手机操作系统、浏览器、嵌入式设备等。SQLite的ACID事务特性保证了数据的完整性和一致性。

（4）性能
对于中小规模的数据量（数十万到数百万条记录），SQLite的性能完全能够满足需求。其读操作性能甚至优于一些客户端-服务器型数据库，因为没有网络通信开销。SQLite的写操作也进行了充分优化，日常使用中不会成为系统瓶颈。

（5）可扩展性
虽然本系统默认使用SQLite，但得益于SQLAlchemy ORM的抽象能力，当企业数据规模增长、需要迁移到MySQL、PostgreSQL等数据库时，只需修改数据库连接配置，业务代码无需改动。这为系统的未来扩展留下了空间。

数据库设计遵循范式理论，避免数据冗余，保证数据一致性。主要包含以下核心表：

- users（用户表）：存储用户账号、密码哈希、昵称、角色、管理员标识、激活状态、创建和更新时间等信息
- model_providers（模型提供商表）：存储LLM提供商信息，包括提供商ID、名称、API地址、图标等
- provider_models（提供商模型表）：存储各提供商可用的模型列表，包括模型ID、名称、是否支持视觉等
- model_configs（模型配置表）：存储用户的LLM配置信息，包括提供商ID、端点URL、API密钥、模型名称、生成参数（temperature、max_tokens、top_p、top_k等）
- chat_sessions（对话会话表）：存储用户的对话会话记录，包括会话标题、创建时间、更新时间等
- chat_messages（对话消息表）：存储会话中的每条消息，包括角色（user/assistant/system）、内容、模型名称、是否流式输出、创建时间等
- system_prompts（系统提示词表）：存储系统级和用户级提示词模板，包括名称、内容、描述、格式类型、分类、是否默认、创建者等
- model_playground_chats（模型测试对话表）：存储多模型对比测试的对话记录，包括会话ID、模型配置ID、角色、内容、推理过程（thinking字段用于存储思维链）等
- test_prompts（测试提示词表）：存储可复用的测试问题模板，供模型测试页面调用
- datasets（数据集表）：存储训练数据集信息，包括名称、描述、文件路径、格式类型、上传者等
- training_configs（训练配置表）：存储训练参数配置，以JSON格式存储完整的配置数据
- training_tasks（训练任务表）：存储模型训练任务信息，包括任务名称、模型名称、数据集ID、配置ID、状态、进度、日志文件、SwanLab地址等

详细的数据库表结构和E-R图将在第三章"系统总体设计"中详细说明。

4. 开发与部署工具

（1）LLaMA-Factory
LLaMA-Factory是一个易于使用的大语言模型训练框架，支持多种主流模型（LLaMA、Qwen、GLM等）的微调。本系统将其集成作为训练引擎，支持通过Web UI和API接口进行模型训练。LLaMA-Factory的灵活性和易用性使得企业用户能够轻松进行模型微调，无需深入了解底层训练细节。

（2）SwanLab
SwanLab是开源的机器学习实验跟踪工具，用于记录训练过程中的损失值、学习率等指标，并提供可视化界面。支持多项目管理和历史记录查询。本系统通过集成SwanLab，为用户提供了直观的训练监控能力，可以实时观察模型训练状态，及时发现和解决问题。

（3）Git
Git是目前最流行的分布式版本控制系统，用于代码管理和团队协作。本系统的源代码使用Git进行版本管理，所有的代码变更都有完整的历史记录，支持分支开发和代码审查，保证了代码质量和团队协作效率。

（4）Uvicorn
Uvicorn是一个基于uvloop和httptools的ASGI服务器，用于运行FastAPI应用。它提供了高性能的异步请求处理能力，是FastAPI官方推荐的生产级服务器。Uvicorn充分利用了Python的异步特性，在高并发场景下表现优异。

（5）Nginx
Nginx是高性能的HTTP服务器和反向代理服务器，在生产环境中用于前端静态文件服务和后端API代理。Nginx的高性能、稳定性和丰富的功能（负载均衡、缓存、SSL/TLS等）使其成为Web应用部署的标准选择。

（五）系统运行环境

系统的运行环境配置直接影响系统的性能和稳定性。本节详细说明系统的软硬件环境要求。

1. 软件环境

表1-5-1 软件环境配置
┌────────────────┬────────────────────────────────────┐
│   名称         │   详细要求                         │
├────────────────┼────────────────────────────────────┤
│ 操作系统       │ Windows 10/11, Linux (Ubuntu 20.04+), │
│                │ macOS 11+                          │
├────────────────┼────────────────────────────────────┤
│ Python环境     │ Python 3.10+                       │
├────────────────┼────────────────────────────────────┤
│ Node.js环境    │ Node.js 16.0+, npm 8.0+            │
├────────────────┼────────────────────────────────────┤
│ 数据库         │ SQLite 3.35+                       │
│                │ (可选MySQL 8.0+/PostgreSQL 13+)    │
├────────────────┼────────────────────────────────────┤
│ 后端服务器     │ Uvicorn 0.20+ / Gunicorn 20.0+     │
├────────────────┼────────────────────────────────────┤
│ 前端构建工具   │ Vite 4.0+                          │
├────────────────┼────────────────────────────────────┤
│ Web服务器      │ Nginx 1.20+ (生产环境)             │
├────────────────┼────────────────────────────────────┤
│ 浏览器         │ Chrome 90+, Firefox 88+,           │
│                │ Edge 90+, Safari 14+               │
└────────────────┴────────────────────────────────────┘

说明：
- 操作系统：系统支持跨平台部署，在Windows、Linux、macOS上均可运行。生产环境推荐使用Linux服务器（如Ubuntu Server 20.04 LTS或CentOS 8+）。
- Python环境：Python 3.10及以上版本，以获得最新的语言特性和性能优化。建议使用虚拟环境（venv或conda）隔离项目依赖。
- Node.js环境：用于前端开发和构建，Node.js 16+提供了对ES模块的完整支持，与Vite配合使用体验最佳。
- 数据库：默认使用SQLite，无需单独安装。如需使用MySQL或PostgreSQL，需单独安装数据库服务器。
- 浏览器：推荐使用现代浏览器的最新版本，以获得最佳的用户体验和性能。

2. 硬件环境

表1-5-2 硬件环境配置
┌────────────────┬────────────────────────────────────┐
│   名称         │   详细要求                         │
├────────────────┼────────────────────────────────────┤
│ 客户端PC配置   │ CPU: 双核2.0GHz+                   │
│                │ 内存: 4GB+                         │
│                │ 硬盘: 100GB+                       │
│                │ 显示器: 1920x1080分辨率            │
│                │ 网络: 宽带接入                     │
├────────────────┼────────────────────────────────────┤
│ 应用服务器配置 │ CPU: 4核3.0GHz+                    │
│                │ 内存: 16GB+                        │
│                │ 硬盘: 500GB+ SSD                   │
│                │ 网卡: 1Gbps                        │
├────────────────┼────────────────────────────────────┤
│ 训练服务器配置 │ CPU: 8核+ (Intel Xeon或AMD EPYC)  │
│ (可选)         │ 内存: 32GB+                        │
│                │ GPU: NVIDIA RTX 3090/4090或A100    │
│                │ 显存: 24GB+                        │
│                │ 硬盘: 1TB+ NVMe SSD                │
├────────────────┼────────────────────────────────────┤
│ 网络环境       │ 内网: 1Gbps (服务器间通信)         │
│                │ 外网: 100Mbps+ (访问云端LLM API)   │
└────────────────┴────────────────────────────────────┘

说明：
- 客户端PC配置：用于访问系统Web界面的用户电脑，配置要求不高。双核处理器和4GB内存即可流畅运行现代浏览器。建议使用1920x1080或更高分辨率的显示器以获得最佳界面显示效果。

- 应用服务器配置：运行系统后端服务和前端静态文件的服务器。4核CPU和16GB内存可以支持约200个并发用户。硬盘推荐使用SSD以提升数据库读写性能。网卡要求千兆以上以保证网络传输速度。

- 训练服务器配置（可选）：仅在需要进行本地模型训练时配置。模型训练对GPU要求较高，推荐使用NVIDIA RTX 3090（24GB显存）或RTX 4090（24GB显存）以上的显卡。对于大型模型（如LLaMA-70B）的训练，建议使用专业级的A100（40GB或80GB显存）。CPU和内存也需要相应提升以支持数据处理。硬盘建议使用NVMe SSD以加快数据读写速度，容量至少1TB以存储模型文件和训练数据。

- 网络环境：内网带宽影响服务器间的通信效率，建议千兆以上。外网带宽影响调用云端LLM API的速度，建议100Mbps以上。如果完全使用本地模型，则对外网带宽无特殊要求。

本章从系统建设背景、研究意义、术语定义、技术选型和运行环境等方面对系统进行了全面综述。明确了系统的建设背景和研究意义，界定了关键术语的含义，阐述了技术选型的理由和优势，并详细说明了系统的软硬件运行环境要求。这些内容为后续章节的需求分析、系统设计和功能实现奠定了基础。

